{
  "book_id": "book_ubkeCZwiahR6nAlEqh40N6NhR4n1_20260102084841",
  "user_id": "ubkeCZwiahR6nAlEqh40N6NhR4n1",
  "metadata": {
    "title": "Unknown",
    "author": "Unknown",
    "total_pages": 121,
    "total_chapters": 1,
    "language": "en"
  },
  "chapters": [
    {
      "number": 1,
      "title": "Full Document",
      "content": "UNIT -1\nCLOUD COMPUTING\nIntroduction\nCloud computing is a type of computing that relies on shared computing resources rather than\nhaving local servers or personal devices to handle applications.\nDefinition by NIST Cloud Computing\nThe National Institute of Stands and Technology(NIST) has a more comprehensive definition\nof cloud computing. It describes cloud computing as \"a model for enabling ubiquitous,\nconvenient, on-demand network access to a shared pool of configurable computing resources\n(e.g., networks, servers, storage, applications and services) that can be rapidly provisioned and\nreleased with minimal management effort or service provider interaction.\"\n\u2022 Ability / space where you store your data ,process it and can access anywhere from the\nworld\n\u2022 As a Metaphor for the internet.\n\uf0fc Cloud computing is :\n\u2022 Storing data /Applications on remote servers\n\u2022 Processing Data / Applications from servers\n\u2022 Accessing Data / Applications via internet\nWhat is a cloud service??\n\uf0fc Cloud computing is taking services and moving them outside an organization's firewall.\nApplications, storage and other services are accessed via the Web. The services are\ndelivered and used over the Internet and are paid for by the cloud customer on an as-\nneeded or pay-per-use business model.\nService: This term in cloud computing is the concept of being able to use reusable, fine-grained\ncomponents across a vendor\u2019s network.\n\uf0b7 Iaas,Paas,Saas,Daas,Naas,Caas are some of the services Provided by different providers\n1.1 Characteristics (OR) Features of Cloud Environments:\nAccording to the NIST, all true cloud environments have five key characteristics:\nK.NIKHILA Page 1\n\nUNIT -1\n1. On-demand self-service: This means that cloud customers can sign up for, pay for and\nstart using cloud resources very quickly on their own without help from a sales agent.\n2. Broad network access: Customers access cloud services via the Internet.\n3. Resource pooling: Many different customers (individuals, organizations or different\ndepartments within an organization) all use the same servers, storage or other computing\nresources.\n4. Rapid elasticity or expansion: Cloud customers can easily scale their use of resources up\nor down as their needs change.\n5. Measured service: Customers pay for the amount of resources they use in a given period\nof time rather than paying for hardware or software upfront. (Note that in a private cloud,\nthis measured service usually involves some form of charge backs where IT keeps track\nof how many resources different departments within an organization are using.)\n1.2 Applications:\ni) Storage:cloud keeps many copies of storage. Using these copies of resources, it extracts\nanother resource if anyone of the resources fails.\nii. Database: are repositories for information with links within the information that help making\nthe data searchable.\nAdvantages:\ni. Improved availability:If there is a fault in one database system, it will only affect\none fragment of the information, not the entire database.\nii. Improved performance: Data is located near the site with the greatest demand and the\ndatabase systems are parallelized, which allows the load to be balanced among the\nservers.\niii. Price It is less expensive to create a network of smaller computers with the power\nof one large one.\niv. Flexibility : Systems can be changed and modified without harm to the entire\ndatabase.\nDisadvantages:\ni. Complexity Database administrators have extra work to do to maintain the\nsystem.\nii. Labor costs With that added complexity comes the need for more workers on the\npayroll.\niii. Security Database fragments must be secured and so must the sites housing the\nfragments.\niv. Integrity It may be difficult to maintain the integrity of the database if it is too\ncomplex or changes too quickly.\nK.NIKHILA Page 2\n\nUNIT -1\nv. Standards There are currently no standards to convert a centralized database into\na cloud solution.\niii. Synchronization\uf0e0allows content to be refreshed across multiple devices.\nEx:\nGoogle docs\nData base services (DaaS): it avoids the complexity and cost of running your own database.\nBenefits:\ni.Ease of use :don\u2019t have to worry about buying, installing, and maintaining hardware\nfor the database as there is no servers to provision and no redundant systems to worry..\nii. Power The database isn\u2019t housed locally, but that doesn\u2019t mean that it is not\nfunctional and effective. Depending on your vendor, you can get custom data\nvalidation to ensure accurate information. You can create and manage the database\nwith ease.\niii. Integration The database can be integrated with your other services to provide\nmore value and power. For instance, you can tie it in with calendars, email, and\npeople to make your work more powerful.\niv. Management because large databases benefit from constant pruning and\noptimization, typically there are expensive resources dedicated to this task. With\nsome DaaS offerings, this management can be provided as part of the service for\nmuch less expense. The provider will often use offshore labor pools to take\nAdvantage of lower labor costs there. So it\u2019s possible that you are using the service\nin Chicago, the physical servers are in Washington state, and the database\nadministrator is in the Philippines.\n\uf0b7 MS SQL and Oracle are two biggest players of DaaS providers.\nMS SQL:\n\uf0b7 Microsoft SQL server data services (SSDS),SSDS based on SQL server, announced\ncloud extension of SQL server tool, in 2008 which is similar to Amazon\u2019s simple\ndatabase (schema \u2013free data storage, SOAP or REST APIs and a pay-as-you-go payment\nsystem.\n\uf0b7 Variation is first, one of the main selling points of SSDS is that it integrates with\nMicrosoft\u2019s sync Framework which is a .NET library for synchronizing dissimilar data\nsources.\n\uf0b7 Microsoft wants SSDS to work as a data hub, synchronizing data on multiple devices so\nthey can be accessed offline.\nCore concepts in SSDS:\ni. Authority \uf0e0 both a billing unit and a collection of containers\nii. Container \uf0e0 collection of entities and is what you search within.\nK.NIKHILA Page 3\n\nUNIT -1\niii. Entity \uf0e0 property bag of name and value pairs\nOracle:\nIt introduces three services to provide database services to cloud users. Customers can license\na. Oracle Database 11g\nb. Oracle fusion Middleware\nc. Oracle enterprise Manager\n\uf0b7 AWS EC2-Amazon web services Elastic Compute cloud\nOracle delivered a set of free Amazon Machine Images (AMIs) to its customers so they could\nquickly and efficiently deploy Oracle\u2019s database solutions.\nDevelopers can take advantage of the provisioning and automated software deployment\nto rapidly build applications using Oracle\u2019s popular development tools such as Oracle\nApplication Express, Oracle Developer, Oracle Enterprise Pack for Eclipse, and Oracle\nWorkshop for Web Logic. Additionally, Oracle Unbreakable Linux Support and AWS\nPremium Support is available for Oracle Enterprise Linux on EC2, providing seamless\ncustomer support.\n\u201cProviding choice is the foundation of Oracle\u2019s strategy to enable customers to become\nmore productive and lower their IT costs\u2014whether it\u2019s choice of hardware, operating\nsystem, or on demand computing\u2014extending this to the Cloud environment is a natural\nevolution,\u201d said Robert Shimp, vice president of Oracle Global Technology Business Unit.\n\u201cWe are pleased to partner with Amazon Web Services to provide our customers enterpriseclass.\nCloud solutions, using familiar Oracle software on which their businesses depend.\u201d\nAdditionally, Oracle also introduced a secure cloud-based backup solution. Oracle\nSecure Backup Cloud Module, based on Oracle\u2019s premier tape backup management\nsoftware, Oracle Secure Backup, enables customers to use the Amazon Simple Storage\nService (Amazon S3) as their database backup destination. Cloud-based backups offer\nreliability and virtually unlimited capacity, available on demand and with no up-front\ncapital expenditure.\nThe Oracle Secure Backup Cloud Module also enables encrypted data backups to help\nensure complete privacy in the cloud environment. It\u2019s fully integrated with Oracle\nRecovery Manager and Oracle Enterprise Manager, providing users with familiar interfaces\nfor cloud-based backups.\nFor customers with an ongoing need to quickly move very large volumes of data into or\nout of the AWS cloud, Amazon allows the creation of network peering connections.\nK.NIKHILA Page 4\n\nUNIT -1\n1.3 Cloud Components:\nThree components of a cloud computing are :\n\u2022 Clients\n\u2022 Data center\n\u2022 Distributed servers\ni. Clients:\n\u2022 Clients are the devices that the end users interact with to manage their information on the\ncloud.\n\u2022 Clients are of three categories :\na. Mobile: mobile devices including PDAs/smart phones like a blackberry, windows, iphone.\nb. Thin: are comps that don\u2019t have internal hard drives then display the info but rather let server\ndo all the work.\nc. Thick: is a regular comp, using web browser like Firefox/Internet Explorer to connect to the\ncloud.\nThin Vs Thick\ni. Price and effect environment\nii. Lower hardware costs\niii. Lower IT costs\niv. Security\nv. Data Security\nvi. Less Power consumption\nvii. Ease of repair or replacement\nviii. Less noise\nii. Data Center :\n\u2022 It is a collection of servers where the application you subscribe and housed.\niii. Distributed Servers:\n\u2022 Servers are in geographically disparate locations but act as if they\u2019re humming away right\nnext to each other.\n\u2022 This gives the service provider more flexibility in options and security.\nK.NIKHILA Page 5\n\nUNIT -1\nEX :\nAmazon has their cloud solution all over the world ,if one failed at one site the service\nwould still be accessed through another site\n\u2022 If cloud needs more h/w they need not throw more servers in the safe room \u2013they can add\nthem at another site and make it part of the cloud.\n1.4 Benefits and Limitations of Cloud Computing\nThe advantage of cloud computing is twofold. It is a file backup shape. It also allows working\non the same document for several jobs (one person or a nomad traveling) of various types (or PC,\ntab or smart phone).\nCloud computing simplifies usage by allowing overcoming the constraints of traditional\ncomputer tools (installation and updating of software, storage, data portability...). Cloud\ncomputing also provides more elasticity and agility because it allows faster access to IT\nresources (server, storage or bandwidth) via a simple web portal and thus without investing in\nadditional hardware.\nConsumers and organizations have many different reasons for choosing to use cloud computing\nservices. They might include the following:\n\uf0b7 Convenience\n\uf0b7 Scalability\n\uf0b7 Low costs\n\uf0b7 Security\n\uf0b7 Anytime, anywhere access\n\uf0b7 High availability\nLimitations /Disadvantages:\nK.NIKHILA Page 6\n\nUNIT -1\na) Down time: Since cloud computing systems are internet-based, service outages are always an\nunfortunate possibility and can occur for any reason.\nBest Practices for minimizing planned downtime in a cloud environment:\nii. Design services with high availability and disaster recovery in mind. Leverage the multi-\navailability zones provided by cloud vendors in your infrastructure.\niii. If your services have a low tolerance for failure, consider multi-region deployments with\nautomated failover to ensure the best business continuity possible.\niv. Define and implement a disaster recovery plan in line with your business objectives that\nprovide the lowest possible recovery time (RTO) and recovery point objectives (RPO).\nv. Consider implementing dedicated connectivity such as AWS Direct Connect, Azure\nExpress Route, or Google Cloud\u2019s Dedicated Interconnect or Partner Interconnect. These\nservices provide a dedicated network connection between you and the cloud service point\nof presence. This can reduce exposure to the risk of business interruption from the public\ninternet.\nb) Security and Privacy: Code Space and the hacking of their AWS EC2 console, which led to\ndata deletion and the eventual shutdown of the company. Their dependence on remote cloud-\nbased infrastructure meant taking on the risks of outsourcing everything.\nBest practices for minimizing security and privacy risks:\n\uf0b7 Understand the shared responsibility model of your cloud provider.\n\uf0b7 Implement security at every level of your deployment.\n\uf0b7 Know who is supposed to have access to each resource and service and limit access to\nleast privilege.\n\uf0b7 Make sure your team\u2019s skills are up to the task: Solid security skills for your cloud teams\nare one of the best ways to mitigate security and privacy concerns in the cloud.\n\uf0b7 Take a risk-based approach to securing assets used in the cloud\nExtend security to the device.\n\uf0b7 Implement multi-factor authentication for all accounts accessing sensitive data or\nsystems.\nc) Vulnerability to Attack: Even the best teams suffer severe attacks and security breaches from\ntime to time.\nBest practices to help you reduce cloud attacks:\n\uf0b7 Make security a core aspect of all IT operations.\n\uf0b7 Keep ALL your teams up to date with cloud security best practices.\n\uf0b7 Ensure security policies and procedures are regularly checked and reviewed.\n\uf0b7 Proactively classify information and apply access control.\n\uf0b7 Use cloud services such as AWS Inspector, AWS CloudWatch, AWS CloudTrail, and\nAWS Config to automate compliance controls.\n\uf0b7 Prevent data ex-filtration.\nK.NIKHILA Page 7\n\nUNIT -1\n\uf0b7 Integrate prevention and response strategies into security operations.\n\uf0b7 Discover rogue projects with audits.\n\uf0b7 Remove password access from accounts that do not need to log in to services.\n\uf0b7 Review and rotate access keys and access credentials.\n\uf0b7 Follow security blogs and announcements to be aware of known attacks.\n\uf0b7 Apply security best practices for any open source software that you are using.\nd) Limited control and flexibility: Since the cloud infrastructure is entirely owned, managed\nand monitored by the service provider, it transfers minimal control over to the customer.\nTo varying degrees (depending on the particular service), cloud users may find they have less\ncontrol over the function and execution of services within a cloud-hosted infrastructure. A cloud\nprovider\u2019s end-user license agreement (EULA) and management policies might impose limits on\nwhat customers can do with their deployments. Customers retain control of their applications,\ndata, and services, but may not have the same level of control over their backend infrastructure.\nBest practices for maintaining control and flexibility:\n\uf0b7 Consider using a cloud provider partner to help with implementing, running, and\nsupporting cloud services.\n\uf0b7 Understanding your responsibilities and the responsibilities of the cloud vendor in the\nshared responsibility model will reduce the chance of omission or error.\n\uf0b7 Make time to understand your cloud service provider\u2019s basic level of support. Will this\nservice level meet your support requirements? Most cloud providers offer additional\nsupport tiers over and above the basic support for an additional cost.\n\uf0b7 Make sure you understand the service level agreement (SLA) concerning the\ninfrastructure and services that you\u2019re going to use and how that will impact your\nagreements with your customers.\ne) Vendor Lock-In: organizations may find it difficult to migrate their services from one vendor\nto another. Differences between vendor platforms may create difficulties in migrating from one\ncloud platform to another, which could equate to additional costs and configuration complexities.\nBest practices to decrease dependency:\n\uf0b7 Design with cloud architecture best practices in mind. All cloud services provide the\nopportunity to improve availability and performance, decouple layers, and reduce\nperformance bottlenecks. If you have built your services using cloud architecture best\npractices, you are less likely to have issues porting from one cloud platform to another.\n\uf0b7 Properly understanding what your vendors are selling can help avoid lock-in challenges.\n\uf0b7 Employing a multi-cloud strategy is another way to avoid vendor lock-in. While this may\nadd both development and operational complexity to your deployments, it doesn\u2019t have to\nbe a deal breaker. Training can help prepare teams to architect and select best-fit services\nand technologies.\nK.NIKHILA Page 8\n\nUNIT -1\n\uf0b7 Build in flexibility as a matter of strategy when designing applications to ensure\nportability now and in the future.\nf) Costs Savings: Adopting cloud solutions on a small scale and for short-term projects can be\nperceived as being expensive.\nBest practices to reduce costs:\n\uf0b7 Try not to over-provision, instead of looking into using auto-scaling services\n\uf0b7 Scale DOWN as well as UP\n\uf0b7 Pre-pay if you have a known minimum usage\n\uf0b7 Stop your instances when they are not being used\n\uf0b7 Create alerts to track cloud spending\n1.5 Architecture\nLet\u2019s have a look into Cloud Computing and see what Cloud Computing is made of. Cloud\ncomputing comprises of two components front end and back end. Front end consist client part\nof cloud computing system. It comprise of interfaces and applications that are required to access\nthe cloud computing platform.\nA central server administers the system, monitoring traffic and client demands to ensure\neverything runs smoothly. It follows a set of rules called protocols and uses a special kind of\nsoftware called MIDDLEWARE. Middleware allows networked computers to communicate\nwith each other. Most of the time, servers don't run at full capacity. That means there's unused\nprocessing power going to waste. It's possible to fool a physical server into thinking it's actually\nmultiple servers, each running with its own independent operating system. The technique is\ncalled server virtualization. By maximizing the output of individual servers, server\nvirtualization reduces the need for more physical machines.\nK.NIKHILA Page 9\n\nUNIT -1\nWhile back end refers to the cloud itself, it comprises of the resources that are required for cloud\ncomputing services. It consists of virtual machines, servers, data storage, security mechanism\netc. It is under provider\u2019s control.\nCloud computing distributes the file system that spreads over multiple hard disks and machines.\nData is never stored in one place only and in case one unit fails the other will take over\nautomatically. The user disk space is allocated on the distributed file system, while another\nimportant component is algorithm for resource allocation. Cloud computing is a strong\ndistributed environment and it heavily depends upon strong algorithm.\nEvolution of Cloud Computing\n\uf0b7 Difficulty Level : Easy\n\uf0b7 Last Updated : 14 May, 2020\nCloud computing is all about renting computing services. This idea first came in the\n1950s. In making cloud computing what it is today, five technologies played a vital role.\nThese are distributed systems and its peripherals, virtualization, web 2.0, service\norientation, and utility computing.\n\uf0b7 Distributed Systems:\nIt is a composition of multiple independent systems but all of them are depicted as a\nsingle entity to the users. The purpose of distributed systems is to share resources and\nalso use them effectively and efficiently. Distributed systems possess characteristics\nsuch as scalability, concurrency, continuous availability, heterogeneity, and\nindependence in failures. But the main problem with this system was that all the\nK.NIKHILA Page 10\n\nUNIT -1\nsystems were required to be present at the same geographical location. Thus to solve\nthis problem, distributed computing led to three more types of computing and they\nwere-Mainframe computing, cluster computing, and grid computing.\n\uf0b7 Mainframe computing:\nMainframes which first came into existence in 1951 are highly powerful and reliable\ncomputing machines. These are responsible for handling large data such as massive\ninput-output operations. Even today these are used for bulk processing tasks such as\nonline transactions etc. These systems have almost no downtime with high fault\ntolerance. After distributed computing, these increased the processing capabilities of\nthe system. But these were very expensive. To reduce this cost, cluster computing\ncame as an alternative to mainframe technology.\nCluster computing:\nIn 1980s, cluster computing came as an alternative to mainframe computing. Each\nmachine in the cluster was connected to each other by a network with high bandwidth.\nThese were way cheaper than those mainframe systems. These were equally capable\nof high computations. Also, new nodes could easily be added to the cluster if it was\nrequired. Thus, the problem of the cost was solved to some extent but the problem\nrelated to geographical restrictions still pertained. To solve this, the concept of grid\ncomputing was introduced.\n\uf0b7 Grid computing:\nIn 1990s, the concept of grid computing was introduced. It means that different\nsystems were placed at entirely different geographical locations and these all were\nconnected via the internet. These systems belonged to different organizations and thus\nthe grid consisted of heterogeneous nodes. Although it solved some problems but new\nproblems emerged as the distance between the nodes increased. The main problem\nwhich was encountered was the low availability of high bandwidth connectivity and\nwith it other network associated issues. Thus. cloud computing is often referred to as\n\u201cSuccessor of grid computing\u201d.\n\uf0b7 Virtualization:\nIt was introduced nearly 40 years back. It refers to the process of creating a virtual\nlayer over the hardware which allows the user to run multiple instances\nsimultaneously on the hardware. It is a key technology used in cloud computing. It is\nthe base on which major cloud computing services such as Amazon EC2, VMware\nK.NIKHILA Page 11\n\nUNIT -1\nvCloud, etc work on. Hardware virtualization is still one of the most common types of\nvirtualization.\n\uf0b7 Web 2.0:\nIt is the interface through which the cloud computing services interact with the clients.\nIt is because of Web 2.0 that we have interactive and dynamic web pages. It also\nincreases flexibility among web pages. Popular examples of web 2.0 include Google\nMaps, Facebook, Twitter, etc. Needless to say, social media is possible because of this\ntechnology only. In gained major popularity in 2004.\n\uf0b7 Service orientation:\nIt acts as a reference model for cloud computing. It supports low-cost, flexible, and\nevolvable applications. Two important concepts were introduced in this computing\nmodel. These were Quality of Service (QoS) which also includes the SLA (Service\nLevel Agreement) and Software as a Service (SaaS).\n\uf0b7 Utility computing:\nIt is a computing model that defines service provisioning techniques for services such\nas compute services along with other major services such as storage, infrastructure,\netc which are provisioned on a pay-per-use basis.\nVirtualization and Cloud Computing\nThe main enabling technology for Cloud Computing is VIRTUALIZATION. Virtualization is a\npartitioning of single physical server into multiple logical servers. Once the physical server is\ndivided, each logical server behaves like a physical server and can run an operating system and\napplications independently. Many popular companies like VMware and Microsoft provide\nvirtualization services, where instead of using your personal PC for storage and computation,\nyou use their virtual server. They are fast, cost-effective and less time consuming.\nFor software developers and testers virtualization comes very handy, as it allows developer to\nwrite code that runs in many different environments and more importantly to test that code.\nVirtualization is mainly used for three main purposes\n1) Network Virtualization\n2) Server Virtualization\nK.NIKHILA Page 12\n\nUNIT -1\n3) Storage Virtualization\nNetwork Virtualization: It is a method of combining the available resources in a network by\nsplitting up the available bandwidth into channels, each of which is independent from the others\nand each channel is independent of others and can be assigned to a specific server or device in\nreal time.\nStorage Virtualization: It is the pooling of physical storage from multiple network storage\ndevices into what appears to be a single storage device that is managed from a central console.\nStorage virtualization is commonly used in storage area networks (SANs).\nServer Virtualization: Server virtualization is the masking of server resources like processors,\nRAM, operating system etc, from server users. The intention of server virtualization is to\nincrease the resource sharing and reduce the burden and complexity of computation from users.\nVirtualization is the key to unlock the Cloud system, what makes virtualization so important for\nthe cloud is that it decouples the software from the hardware. For example, PC\u2019s can use virtual\nmemory to borrow extra memory from the hard disk. Usually hard disk has a lot more space than\nmemory. Although virtual disks are slower than real memory, if managed properly the\nsubstitution works perfectly. Likewise, there is software which can imitate an entire computer,\nwhich means 1 computer can perform the functions equals to 20 computers.\n1.6 Classification of Cloud Variants:\ni. Service Model Based\nii. Deployment Model Based\n1.6.1 Service Model Based /Models Service / Types of Models\nCloud computing services are divided into three classes, according to the abstraction level of the\ncapability provided and the service model of providers, namely:\n1. Infrastructure as a Service (IaaS)\n2. Platform as a Service (PaaS) and\n3. Software as a Service. (SaaS)\nThese abstraction levels can also be viewed as a layered architecture where services of a higher\nlayer can be composed from services of the underlying layer. The reference model explains the\nrole of each layer in an integrated architecture. A core middleware manages physical resources\nand the VMs deployed on top of them; in addition, it provides the required features (e.g.,\naccounting and billing) to offer multi-tenant pay-as-you-go services.\nK.NIKHILA Page 13\n\nUNIT -1\nCloud development environments are built on top of infrastructure services to offer application\ndevelopment and deployment capabilities; in this level, various programming models, libraries,\nAPIs, and mashup editors enable the creation of a range of business, Web, and scientific\napplications. Once deployed in the cloud, these applications can be consumed by end users.\nINFRASTRUCTURE AS A SERVICE\nOffering virtualized resources (computation, storage, and communication) on demand is known\nas Infrastructure as a Service (IaaS).\nFIGURE 1.3. The cloud computing stack.\nA cloud infrastructure enables on-demand provisioning of servers running several choices of\noperating systems and a customized software stack. Infrastructure services are considered to be\nthe bottom layer of cloud computing systems.\n\uf0b7 Amazon Web Services mainly offers IaaS, which in the case of its EC2 service means\noffering VMs with a software stack that can be customized similar to how an ordinary\nphysical server would be customized.\n\uf0b7 Users are given privileges to perform numerous activities to the server, such as: starting\nand stopping it, customizing it by installing software packages, attaching virtual disks to\nit, and configuring access permissions and firewalls rules.\nK.NIKHILA Page 14\n\nUNIT -1\nPLATFORM AS A SERVICE\nIn addition to infrastructure-oriented clouds that provide raw computing and storage services,\nanother approach is to offer a higher level of abstraction to make a cloud easily programmable,\nknown as Platform as a Service (PaaS).\nA cloud platform offers an environment on which developers create and deploy\napplications and do not necessarily need to know how many processors or how much memory\nthat applications will be using. In addition, multiple programming models and specialized\nservices (e.g., data access, authentication, and payments) are offered as building blocks to new\napplications.\nGoogle App Engine, an example of Platform as a Service, offers a scalable environment for\ndeveloping and hosting Web applications, which should be written in specific programming\nlanguages such as Python or Java, and use the services\u2018 own proprietary structured object data\nstore. Building blocks include an in-memory object cache (mem cache), mail service, instant\nmessaging service (XMPP), an image manipulation service, and integration with Google\nAccounts authentication service. Software as a Service Applications reside on the top of the\ncloud stack. Services provided by this layer can be accessed by end users through Web portals.\nTherefore, consumers are increasingly shifting from locally installed computer programs to on-\nline software services that offer the same functionally. Traditional desktop applications such as\nword processing and spreadsheet can now be accessed as a service in the Web. This model of\ndelivering applications, known as Software as a Service (F), alleviates the burden of software\nmaintenance for customers and simplifies development and testing for providers.\nSalesforce.com, which relies on the SaaS model, offers business productivity applications\n(CRM) that reside completely on their servers, allowing customers to customize and access\napplications on demand.\nK.NIKHILA Page 15\n\nUNIT -1\nc. Infrastructure as a Service (IaaS) or Hardware as a Service (HaaS)\nINFRASTRUCTURE AS A SERVICE PROVIDERS\nPublic Infrastructure as a Service providers commonly offer virtual servers containing\none or more CPUs, running several choices of operating systems and a customized software\nstack.\nFEATURES\nThe most relevant features are:\ni. Geographic distribution of data centers;\nii. Variety of user interfaces and APIs to access the system;\na. Specialized components and services that aid particular applications (e.g.,\nload- balancers, firewalls);\nb. Choice of virtualization platform and operating systems; and\nc. Different billing methods and period (e.g., prepaid vs. postpaid, hourly vs.\nmonthly).\nGeographic Presence:\nAvailability zones are \u2015distinct locations that are\uf0b7 engineered to be insulated from failures in\nother availability zones and provide inexpensive, low-latency network connectivity to other\nK.NIKHILA Page 16\n\nUNIT -1\navailability zones in the same region. Regions, in turn, \u2015are geographically dispersed and will\nbe in separate geographic areas or countries.\nUser Interfaces And Access To Servers:\nA public IaaS provider must provide multiple access means\uf0b7 to its cloud, thus catering for\nvarious users and their preferences. Different types of user interfaces (UI) provide different\nlevels of abstraction, the most common being graphical user interfaces (GUI), command-line\ntools (CLI), and Web service (WS) APIs. GUIs are preferred by end users who need to launch,\uf0b7\ncustomize, and monitor a few virtual servers and do not necessary need to repeat the process\nseveral times.\nAdvance Reservation Of Capacity:\nAdvance reservations allow users to request for an IaaS\uf0b7 provider to reserve resources for a\nspecific time frame in the future, thus ensuring that cloud resources will be available at that\ntime. Amazon Reserved Instances is a form of advance\uf0b7 reservation of capacity, allowing users\nto pay a fixed amount of money in advance to guarantee resource availability at anytime during\nan agreed period and then paying a discounted hourly rate when resources are in use.\nAutomatic Scaling And Load Balancing:\nIt allow users to set conditions for when they want their\uf0b7 applications to scale up and down,\nbased on application specific metrics such as transactions per second, number of simultaneous\nusers, request latency, and so forth. When the number of virtual servers is increased by\uf0b7\nautomatic scaling, incoming traffic must be automatically distributed among the available\nservers. This activity enables applications to promptly respond to traffic increase while also\nachieving greater fault tolerance.\nService-Level Agreement:\nService-level agreements (SLAs) are offered by IaaS\uf0b7 providers to express their commitment to\ndelivery of a certain QoS. To customers it serves as a warranty. An SLA usually include\navailability and performance guarantees. HYPERVISOR AND OPERATING SYSTEM\nCHOICE: IaaS offerings have been based on heavily customized\uf0b7 open-source Xen\ndeployments. IaaS providers needed expertise in Linux, networking, virtualization, metering,\nresource management, and many other low-level aspects to successfully deploy and maintain\ntheir cloud offerings.\nK.NIKHILA Page 17\n\nUNIT -1\nPaaS Providers\nPublic Platform as a Service providers commonly offer a development and deployment\nenvironment that allow users to create and run their applications with little or no concern to\nlow-level details of the platform.\nFEATURES\nProgramming Models, Languages, and Frameworks. Programming models made available by\nIaaS providers define how users can express their applications using higher levels of abstraction\nand efficiently run them on the cloud platform and recover it in case of crashes, as well as to\nstore user data.\nPersistence Options. A persistence layer is essential to allow applications to record their state\nand recover it in case of crashes, as well as to store user data.\n1.6.2 Deployment Model Based/Types of CC / Cloud Delivery Models:\nCloud computing can be divided into several sub-categories depending on the physical location\nof the computing resources and who can access those resources.\na. Public cloud vendors offer their computing services to anyone in the general public. They\nmaintain large data centers full of computing hardware, and their customers share access to that\nhardware.\nb. Private cloud is a cloud environment set aside for the exclusive use of one organization. Some\nlarge enterprises choose to keep some data and applications in a private cloud for security\nreasons, and some are required to use private clouds in order to comply with various regulations.\nOrganizations have two different options for the location of a private cloud: they can set up a\nprivate cloud in their own data centers or they can use a hosted private cloud service. With a\nhosted private cloud, a public cloud vendor agrees to set aside certain computing resources and\nallow only one customer to use those resources.\nc. Hybrid cloud is a combination of both a public and private cloud with some level of\nintegration between the two. For example, in a practice called \"cloud bursting\" a company may\nrun Web servers in its own private cloud most of the time and use a public cloud service for\nadditional capacity during times of peak use.\nA multi-cloud environment is similar to a hybrid cloud because the customer is using more than\none cloud service. However, a multi-cloud environment does not necessarily have integration\namong the various cloud services, the way a hybrid cloud does. A multi-cloud environment can\nK.NIKHILA Page 18\n\nUNIT -1\ninclude only public clouds, only private clouds or a combination of both public and private\nclouds.\nd. Community Cloud: Here, computing resources are provided for a community and\norganizations.\n1.7 Infrastructure of Cloud Computing\n\uf0b7 Cloud infrastructure means the hardware and software components.\n\uf0b7 These components are server, storage, and networking and virtualization software.\n\uf0b7 These components are required to support the computing requirements of a cloud computing\nmodel.\nComponents of Cloud infrastructure\na) Hypervisor\n\uf0b7 Hypervisor is a firmware or low-level program. It acts as a Virtual Machine Manager.\n\uf0b7 It enables to share a physical instance of cloud resources between several customers.\nb) Management Software\n\uf0b7 Management software assists to maintain and configure the infrastructure.\nc) Deployment Software\n\uf0b7 Deployment software assists to deploy and integrate the application on the cloud.\nd) Network\n\uf0b7 Network is the key component of the cloud infrastructure.\n\uf0b7 It enables to connect cloud services over the Internet.\n\uf0b7 The customer can customize the network route and protocol i.e possible to deliver network as a\nutility over the Internet.\ne) Server\n\uf0b7 The server assists to compute the resource sharing and offers other services like resource\nallocation and de-allocation, monitoring the resources, provides the security etc.\n6) Storage\nK.NIKHILA Page 19\n\nUNIT -1\n\uf0b7 Cloud keeps many copies of storage. Using these copies of resources, it extracts another\nresource if any one of the resources fails.\nIntranets and the Cloud: Intranets are customarily used within an organization and are not\naccessible publicly. That is, a web server is maintained in-house and company information is\nmaintained on it that others within the organization can access. However, now intranets are being\nmaintained on the cloud.\n\uf0b7 To access the company\u2019s private, in-house information, users have to log on to the\nintranet by going to a secure public web site.\n\uf0b7 There are two main components in client/server computing: servers and thin or light\nclients.\n\uf0b7 The servers house the applications your organization needs to run, and the thin\nClients\u2014who do not have hard drives\u2014display the results.\nHypervisor Applications\n\uf0b7 Applications like VMware or Microsoft\u2019s Hyper-V allow you to virtualize your servers\nso\nthat multiple virtual servers can run on one physical server.\n\uf0b7 These sorts of solutions provide the tools to supply a virtualized set of hardware to the\nguest operating system. They also make it possible to install different operating systems\non the same machine. For example, you may need Windows Vista to run one application,\nwhile another application requires Linux. It\u2019s easy to set up the server to run both\noperating systems.\n\uf0b7 Thin clients use an application program to communicate with an application server.\nMost of the processing is done down on the server, and sent back to the client.\nThere is some debate about where to draw the line when talking about thin clients.\nSome thin clients require an application program or a web browser to communicate with\nthe server. However, others require no add-on applications at all. This is sort of a discussion of\nsemantics, because the real issue is whether the work is being done on the server and transmitted\nback to the thin client.\n1.8. Cloud computing techniques\nSome traditional computing techniques that have helped enterprises achieve additional\ncomputing and storage capabilities, while meeting customer demands using shared\nphysical resources, are:\n\uf0b7 Cluster computing connects different computers in a single location via LAN to work as\na single computer. Improves the combined performance of the organization which owns\nit\nK.NIKHILA Page 20\n\nUNIT -1\n\uf0b7 Grid computing enables collaboration between enterprises to carry out distributed\ncomputing jobs using interconnected computers spread across multiple locations running\nindependently\n\uf0b7 Utility computing provides web services such as computing, storage space, and\napplications to users at a low cost through the virtualization of several backend servers.\nUtility computing has laid the foundation for today\u2019s cloud computing\n\uf0b7 Distributed computing landscape connects ubiquitous networks and connected devices\nenabling peer-to-peer computing. Examples of such cloud infrastructure are ATMs, and\nintranets/ workgroups\nGrid Computing Vs Cloud Computing\nWhen we switch on the fan or any electric device, we are less concern about the power supply\nfrom where it comes and how it is generated. The power supply or electricity that we receives at\nour home travels through a chain of network, which includes power stations, transformers, power\nlines and transmission stations. These components together make a \u2018Power Grid\u2019. Likewise,\n\u2018Grid Computing\u2019 is an infrastructure that links computing resources such as PCs, servers,\nworkstations and storage elements and provides the mechanism required to access them.\nGrid Computing is a middle ware to co-ordinate disparate IT resources across a network,\nallowing them to function as whole. It is more often used in scientific research and in universities\nfor educational purpose. For example, a group of architect students working on a different\nproject requires a specific designing tool and a software for designing purpose but only couple of\nthem got access to this designing tool, the problem is how they can make this tool available to\nrest of the students. To make available for other students they will put this designing tool on\ncampus network, now the grid will connect all these computers in campus network and allow\nstudent to use designing tool required for their project from anywhere. Cloud computing and\nGrid computing is often confused, though there functions are almost similar there approach for\ntheir functionality is different. Let see how they operate-\nCloud Computing Grid Computing\n\uf0b7 Cloud computing works more as a \uf0b7 Grid computing uses the available\nservice provider for utilizing computer resource and interconnected computer\nresource systems to accomplish a common goal\n\uf0b7 Grid computing is a decentralized\n\uf0b7 Cloud computing is a centralized model model, where the computation could\noccur over many administrative model\nK.NIKHILA Page 21\n\nUNIT -1\n\uf0b7 A grid is a collection of computers\n\uf0b7 Cloud is a collection of computers which is owned by a multiple parties in\nusually owned by a single party. multiple locations and connected\n\uf0b7 together so that users can share the\ncombined power of resources\n\uf0b7 Cloud offers more services all most all\nthe services like web hosting, DB (Data \uf0b7 Grid provides limited services\nBase) support and much more\n\uf0b7 Cloud computing is typically provided\n\uf0b7 Grid computing federates the resources\nwithin a single organization (eg :\nlocated within different organization.\nAmazon)\nUtility Computing Vs Cloud Computing\nIn our previous conversation in \u201cGrid Computing\u201d we have seen how electricity is supplied to\nour house, also we do know that to keep electricity supply we have to pay the bill. Utility\nComputing is just like that, we use electricity at home as per our requirement and pay the bill\naccordingly likewise you will use the services for the computing and pay as per the use this is\nknown as \u2018Utility computing\u2019. Utility computing is a good source for small scale usage, it can be\ndone in any server environment and requires Cloud Computing.\nUtility computing is the process of providing service through an on-demand, pay per use billing\nmethod. The customer or client has access to a virtually unlimited supply of computing solutions\nover a virtual private network or over the internet, which can be sourced and used whenever it\u2019s\nrequired. Based on the concept of utility computing , grid computing, cloud computing and\nmanaged IT services are based.\nThrough utility computing small businesses with limited budget can easily use software like\nCRM (Customer Relationship Management) without investing heavily on infrastructure to\nmaintain their clientele base.\nUtility Computing Cloud Computing\n\uf0b7 Utility computing refers to the ability to \uf0b7 Cloud Computing also works like utility\ncharge the offered services, and charge computing, you pay only for what you\ncustomers for exact usage use but Cloud Computing might be\ncheaper, as such, Cloud based app can\nK.NIKHILA Page 22\n\nUNIT -1\nbe up and running in days or weeks.\n\uf0b7 Utility computing users want to be in \uf0b7 In cloud computing, provider is in\ncontrol of the geographical location of complete control of cloud computing\nthe infrastructure services and infrastructure\n\uf0b7 Utility computing is more favorable \uf0b7 Cloud computing is great and easy to\nwhen performance and selection use when the selection infrastructure\ninfrastructure is critical and performance is not critical\n\uf0b7 Utility computing is a good choice for \uf0b7 Cloud computing is a good choice for\nless resource demanding high resource demanding\n\uf0b7 Utility computing refers to a business \uf0b7 Cloud computing refers to the\nmodel underlying IT architecture\n1.9 Security concerns for Cloud Computing\nWhile using cloud computing, the major issue that concerns the users is about its security.\nOne concern is that cloud providers themselves may have access to customer\u2019s. unencrypted\ndata- whether it\u2019s on disk, in memory or transmitted over the network. Some countries\ngovernment may decide to search through data without necessarily notifying the data owner,\ndepending on where the data resides, which is not appreciated and is considered as a privacy\nbreach (Example Prism Program by USA).\nTo provide security for systems, networks and data cloud computing service providers have\njoined hands with TCG (Trusted Computing Group) which is non-profit organization which\nregularly releases a set of specifications to secure hardware, create self-encrypting drives and\nimprove network security. It protects the data from root kits and malware.\nAs computing has expanded to different devices like hard disk drives and mobile phones, TCG\nhas extended the security measures to include these devices. It provides ability to create a unified\ndata protection policy across all clouds.\nSome of the trusted cloud services are Amazon, Box.net, Gmail and many others.\n1.10 Privacy Concern & Cloud Computing\nPrivacy presents a strong barrier for users to adapt into Cloud Computing systems\nK.NIKHILA Page 23\n\nUNIT -1\nThere are certain measures which can improve privacy in cloud computing.\n1. The administrative staff of the cloud computing service could theoretically monitor the\ndata moving in memory before it is stored in disk. To keep the confidentiality of a data,\nadministrative and legal controls should prevent this from happening.\n2. The other way for increasing the privacy is to keep the data encrypted at the cloud storage\nsite, preventing unauthorized access through the internet; even cloud vendor can\u2019t access\nthe data either.\nii) Full Virtualization\n\uf0b7 Full virtualization is a technique in which a complete installation of one machine is run\non another. The result is a system in which all software running on the server is within a\nvirtual machine.\n\uf0b7 In a fully virtualized deployment, the software running on the server is displayed on the\nclients.\n\uf0b7 Virtualization is relevant to cloud computing because it is one of the ways in which you\nwill access services on the cloud. That is, the remote datacenter may be delivering your\nservices in a fully virtualized format.\n\uf0b7 In order for full virtualization to be possible, it was necessary for specific hardware\ncombinations to be used. It wasn\u2019t until 2005 that the introduction of the AMD-\nVirtualization(AMD-V) and Intel Virtualization Technology (IVT) extensions made it easier to\ngo fully virtualized.\nFull virtualization has been successful for several purposes:\ni) Sharing a computer system among multiple users\nii) Isolating users from each other and from the control program\niii) Emulating hardware on another machine\niii) Para virtualization\nPara virtualization allows multiple operating systems to run on a single hardware device at\nthe same time by more efficiently using system resources, like processors and memory.\nIn full virtualization, the entire system is emulated (BIOS, drive, and so on), but in\npara virtualization, its management module operates with an operating system that has\nbeen adjusted to work in a virtual machine. Para virtualization typically runs better than the full\nvirtualization model, simply because in a fully virtualized deployment, all elements\nmust be emulated.\nK.NIKHILA Page 24\n\nUNIT -1\nCHALLENGES AND RISKS\nDespite the initial success and popularity of the cloud computing paradigm and the extensive\navailability of providers and tools, a significant number of challenges and risks are inherent to\nthis new model of computing. Providers, developers, and end users must consider these\nchallenges and risks to take good advantage of cloud computing. Issues to be faced include user\nprivacy, data security, data lock- in, availability of service, disaster recovery, performance,\nscalability, energy- efficiency, and programmability. Security, Privacy, and Trust: Security and\nprivacy affect the entire cloud computing stack, since there is a massive use of third- party\nservices and infrastructures that are used to host important data or to perform critical operations.\nIn this scenario, the trust toward providers is fundament al to ensure the desired level of privacy\nfor applications hosted in the cloud. | 62 Legal and regulatory issues also need attention. When\ndata are moved into the Cloud, providers may choose to locate them anywhere on the planet. The\nphysical location of data centers determines the set of laws that can be applied to the\nmanagement of data. For example, specific cryptography techniques could not be used because\nthey are not allowed in some countries. Similarly, country laws can impose that sensitive data,\nsuch as patient health records, are to be stored within national borders. Data Lock- In and\nStandardization: A major concern of cloud computing users is about having their data locked- in\nby a certain provider. Users may want to move data and applications out from a provider that\ndoes not meet their requirements. However, in their current form, cloud computing\ninfrastructures and platforms do not employ standard methods of storing user data and\napplications. Consequently, they do not interoperate and user data are not portable. The answer\nK.NIKHILA Page 25\n\nUNIT -1\nto this concern is standardization. In this direction, there are efforts to create open standard s for\ncloud computing. The Cloud Computing Interopera bility Forum (CCIF) was formed by\norganizations such as Intel, Sun, and Cisco in order to \u201cenable a global cloud computing\necosystem whereby organizations are able to seamlessly work together for the purposes for wider\nindustry adoption of cloud computing technology.\u201d The development of the Unified Cloud\nInterface (UCI) by CCIF aims at creating a standard programmatic point of access to an entire\ncloud infrastructure. In the hardware virtualization sphere, the Open Virtual Format (OVF) aims\nat facilitating packing and distribution of software to be run on VMs so that virtual appliances\ncan be made portable\u2014that is, seamlessly run on hypervisor of different vendors. Availability,\nFault- Tolerance, and Disaster Recovery: It is expected that users will have certain expectations\nabout the service level to be provided once their applications are moved to the cloud. These\nexpectations include availability of the service, its overall performance, and what measures are to\nbe taken when something goes wrong in the system or its component s. In summary, users seek\nfor a warranty before they can comfortably move their business to the cloud. SLAs, which\ninclude QoS requirements, must be ideally set up between customers and cloud computing\nproviders to act as warranty. An SLA specifies the details of the service to be provided, including\navailability and performance guarantees. Additionally, metrics must be agreed upon by all\nparties, and penalties for violating the expectations must also be approved. Resource\nManagement and Energy- Efficiency: One important challenge faced by providers of cloud\ncomputing services is the efficient manage m e n t of virtualized resource pools. Physical\nresources such as CPU cores, disk space, and network bandwidth must be sliced and shared\namong virtual machines running potentially heterogeneous workloads. The multidimensional\nnature of virtual machines complicates the activity of finding a good mapping of VMs onto\navailable physical hosts while maximizing user | 63 utility. Dimensions to be considered include:\nnumber of CPUs, amount of memory, size of virtual disks, and network bandwidth. Dynamic\nVM mapping policies may leverage the ability to suspend, migrate, and resume VMs as an easy\nway of preempting low- priority allocations in favor of higher- priority ones. Migration of VMs\nalso brings additional challenges such as detecting when to initiate a migration, which VM to\nmigrate, and where to migrate. In addition, policies may take advantage of live migration of\nvirtual machines to relocate data center load without significantly disrupting running services. In\nthis case, an additional concern is the tradeoff between the negative impact of a live migration on\nthe performance and stability of a service and the benefits to be achieved with that migration.\nAnother challenge concerns the outstanding amount of data to be managed in various VM\nmanage m e n t activities. Such data amount is a result of particular abilities of virtual machines,\nincluding the ability of traveling through space (i.e., migration) and time (i.e., check pointing and\nrewinding), operations that may be required in load balancing, backup, and recovery scenarios.\nIn addition, dynamic provisioning of new VMs and replicating existing VMs require efficient\nmechanisms to make VM block storage devices (e.g., image files) quickly available at selected\nhosts. Data centers consume r large amounts of electricity. According to a data published by\nHP[4], 100 server racks can consume 1.3MWof power and another 1.3 MW are required by the\nK.NIKHILA Page 26\n\nUNIT -1\ncooling system, thus costing USD 2.6 million per year. Besides the monetary cost, data centers\nsignificantly impact the environment in terms of CO2 emissions from the cooling systems\nIssues in cloud:\nThe Eucalyptus : framework was one of the first open- source projects to focus on building IaaS\nclouds. It has been developed with the intent of providing an open- source implement a tion\nnearly identical in functionality to Amazon Web Services APIs. Eucalyptus provides the\nfollowing features: Linux- based controller with administration Web portal; EC2- compatible\n(SOAP, Query) and S3- compatible (SOAP, REST) CLI and Web portal interfaces; Xen, KVM,\nand VMWare backends; Amazon EBS- compatible virtual storage devices; interface to the\nAmazon EC2 public cloud; virtual networks.\nNimbus3: The Nimbus toolkit is built on top of the Globus framework. Nimbus provides most\nfeatures in common with other open- source VI managers, such as an EC2- compatible front- end\nAPI, support to Xen, and a backend interface to Amazon EC2. However, it distinguishes from\nothers by providing a Globus Web Services Resource Framework (WSRF) interface. It also\nprovides a backend service, named Pilot, which spawns VMs on clusters manage d by a local\nresource manager (LRM) such as PBS and SGE.\nOpen Nebula: Open Nebula is one of the most feature- rich open- source VI managers. It was\ninitially conceived to manage local virtual infrastructure, but has also included remote interfaces\nthat make it viable to build public clouds. Altogether r, four programming APIs are available:\nXML-RPC and libvirt for local interaction; a subset of EC2 (Query) APIs and the Open Nebula\nCloud API (OCA) for public access. Open Nebula provides the following features: Linux- based\ncontroller; CLI, XML-RPC, EC2- compatible Query and OCA interfaces; Xen, KVM, and\nVMware backend; interface to public clouds (Amazon EC2, Elastic Hosts); virtual networks;\ndynamic resource allocation; advance reservation of capacity.\nCASE STUDY\nThe Eucalyptus :\nframework was one of the first open- source projects to focus on building IaaS clouds. It has\nbeen developed with the intent of providing an open- source implement a tion nearly identical in\nfunctionality to Amazon Web Services APIs. Eucalyptus provides the following features: Linux-\nbased controller with administration Web portal; EC2- compatible (SOAP, Query) and S3-\ncompatible (SOAP, REST) CLI and Web portal interfaces; Xen, KVM, and VMWare backends;\nAmazon EBS- compatible virtual storage devices; interface to the Amazon EC2 public cloud;\nvirtual networks.\nK.NIKHILA Page 27\n\nUNIT -1\nNimbus3 : The Nimbus toolkit is built on top of the Globus framework. Nimbus provides most\nfeatures in common with other open- source VI manage rs, such as an EC2- compatible front-\nend API, support to Xen, and a backend interface to Amazon EC2. However, it distinguishes\nfrom others by providing a Globus Web Services Resource Framework (WSRF) interface. It also\nprovides a backend service, named Pilot, which spawns VMs on clusters manage d by a local\nresource manager (LRM) such as PBS and SGE.\nOpen Nebula:\nOpen Nebula is one of the most feature- rich open- source VI managers. It was initially\nconceived to manage local virtual infrastructure, but has also included remote interfaces that\nmake it viable to build public clouds. Altogether, four programming APIs are available: XML-\nRPC and libvirt for local interaction; a subset of EC2 (Query) APIs and the OpenNebula Cloud\nAPI (OCA) for public access. OpenNebula provides the following features: Linux- based\ncontroller; CLI, XML-RPC, EC2- compatible Query and OCA interfaces; Xen, KVM, and\nVMware backend; interface to public clouds (Amazon EC2, ElasticHosts); virtual networks;\ndynamic resource allocation; advance reservation of capacity.\ni) Case-Study of Cloud Computing- Royal Mail\n\uf0b7 Subject of Case-Study: Using Cloud Computing for effective communication among\nstaff.\n\uf0b7 Reason for using Cloud Computing: Reducing the cost made after communication for\n28,000 employees and to provide advance features and interface of e-mail services to\ntheir employees.\nRoyal mail group, a postal service in U.K, is the only government organization in U.K that\nserves over 24 million customers through its 12000 post offices and 3000 separate processing\nsites. Its logistics systems and parcel-force worldwide handles around 404 million parcel a year.\nAnd to do this they need an effective communicative medium. They have recognized the\nadvantage of Cloud Computing and implemented it to their system. It has shown an outstanding\nperformance in inter-communication.\nBefore moving on to Cloud system, the organization was struggling with the out-of-date\nsoftware, and due to which the operational efficiency was getting compromised. As soon as the\norganization switched on to Cloud System, 28000 employees were supplied with their new\ncollaboration suite, giving them access to tools such as instant messaging and presence\nawareness. The employees got more storage place than on local server. The employees became\nmuch more productive.\nK.NIKHILA Page 28\n\nUNIT -1\nLooking to the success of Cloud Computing in e-mail services and communication .The second\nstrategic move of Royal Mail Group, was to migrating from physical servers to virtual servers,\nup to 400 servers to create a private cloud based on Microsoft hyper V. This would give a fresh\nlook and additional space to their employees desktop and also provides latest modern exchange\nenvironment.\nThe hyper V project by RMG\u2019s (Royal Mail Group) is estimated to save around 1.8 million\npound for them in future and will increase the efficiency of the organization\u2019s internal IT system.\nCase study -2\nXYZ is a startup IT organization that develops and sells s/w the org gets a new website\ndevelopment project that needs a web server, application server and a database server. The\norg has hired 30 employees for this web development project.\nConstraints :\nAcquiring renting space for new servers\nBuying new high end servers\nHiring new IT staff for infrastructure management\nBuying licensed OS and other s/w required for development\nSolution :Public cloud IaaS\nTeam leader :\n1. Creates an ac\n2. Choose an VM image from image repository or create a new image\n3. Specify no.of VM\u2019s\n4. Choose VM type\n5. Set necessary configurations for VM\n6. After VM launched ,provide IP address of VM to prog team\n7. Access VM and start development\nCase study -2\nCase study -3\nXYZ firm gets more revenue ,grows and hence buys some IT infrastructuire.However it\ncontinues to use public IaaS cloud for its development work\nNow the firm gets a new project that involves sensitive data that restricts the firm to use a\npublic cloud .hence this org is in need of setting up the required infrastructure in its own\npremise.\nConstraints:\nInfrastructure cost\nK.NIKHILA Page 29\n\nUNIT -1\nInfrastructure optimization\nPower consumption\nData center management\nAdditional expenditure on infrastructure operation with lesser productivity\nSolution : Private IaaS cloud\nMoving to private cloud is :\nMoving to private cloud\nIT managed \uf0e0 self-service\nPhysical \uf0e0 virtual\nManual management \uf0e0 automated management\nDedicated \uf0e0 shared\nExplanation:\n1.Setup cloud infrastructure\n2. Setup self-service portal or dashboard\n3. Test the cloud environment through self-service\n4. Get VM\u2019s\n5. Use VM\u2019s to develop and test applications\n6. Manage cloud environment\nCloud Computing Service Provider Companies in 2019\n1) Amazon Web Services\nK.NIKHILA Page 30\n\nUNIT -1\nAWS is Amazon's cloud web hosting platform which offers fast, flexible, reliable and cost-\neffective solutions. It offers a service in the form of building block which can be used to create\nand deploy any kind of application in the cloud. It is the most popular as it was the first to enter\nthe cloud computing space.\nFeatures:\n\uf0b7 Easy sign-up process\n\uf0b7 Fast Deployments\n\uf0b7 Allows easy management of add or remove capacity\n\uf0b7 Access to effectively limitless capacity\n\uf0b7 Centralized Billing and management\n\uf0b7 Offers Hybrid Capabilities and per hour billing\nDownload link:https://aws.amazon.com/\n2) Microsoft Azure\nAzure is a cloud computing platform which is launched by Microsoft in February 2010. This\nopen source and flexible cloud platform which helps in development, data storage, service\nmanagement & hosting solutions.\nFeatures:\n\uf0b7 Windows Azure offers the most effective solution for your data needs\n\uf0b7 Provides scalability, flexibility, and cost-effectiveness\n\uf0b7 Offers consistency across clouds with familiar tools and resources\n\uf0b7 Allow you to scale your IT resources up and down according to your business needs\nDownload link:https://azure.microsoft.com/en-in/\nK.NIKHILA Page 31\n\nUNIT -1\n3) Google Cloud Platform\nGoogle Cloud is a set of solution and products which includes GCP & G suite. It helps you to\nsolve all kind of business challenges with ease.\nFeatures:\n\uf0b7 Allows you to scale with open, flexible technology\n\uf0b7 Solve issues with accessible AI & data analytics\n\uf0b7 Eliminate the need for installing costly servers\n\uf0b7 Allows you to transform your business with a full suite of cloud-based services\nDownload link:https://cloud.google.com/\n4) VMware\nVMware is a comprehensive cloud management platform. It helps you to manage a hybrid\nenvironment running anything from traditional to container workloads. The tools also allow you\nto maximize the profits of your organization.\nFeatures:\n\uf0b7 Enterprise-ready Hybrid Cloud Management Platform\n\uf0b7 Offers Private & Public Clouds\n\uf0b7 Comprehensive reporting and analytics which improve the capacity of forecasting &\nplanning\n\uf0b7 Offers additional integrations with 3rd parties and custom applications, and tools.\n\uf0b7 Provides flexible, Agile services\nDownload link:https://www.vmware.com/in/cloud-services/infrastructure.html\nK.NIKHILA Page 32\n\nUNIT -1\nOracle Cloud\nOracle Cloud offers innovative and integrated cloud services. It helps you to build, deploy, and\nmanage workloads in the cloud or on premises. Oracle Cloud also helps companies to transform\ntheir business and reduce complexity.\nFeatures:\n\uf0b7 Oracle offers more options for where and how you make your journey to the cloud\n\uf0b7 Oracle helps you realize the importance of modern technologies including Artificial\nintelligence, chatbots, machine learning, and more\n\uf0b7 Offers Next-generation mission-critical data management in the cloud\n\uf0b7 Oracle provides better visibility to unsanctioned apps and protects against sophisticated\ncyber attacks\nDownload link:https://www.oracle.com/cloud/\n5) IBM Cloud\nIBM cloud is a full stack cloud platform which spans public, private and hybrid environments. It\nis built with a robust suite of advanced and AI tools.\nFeatures:\n\uf0b7 IBM cloud offers infrastructure as a service (IaaS), software as a service (SaaS) and\nplatform as a service (PaaS)\n\uf0b7 IBM Cloud is used to build pioneering which helps you to gain value for your businesses\n\uf0b7 It offers high performing cloud communications and services into your IT environment\nDownload link:https://www.ibm.com/cloud/\nTips for selecting a Cloud Service Provider\nThere \"best\" Cloud Service cannot be defined. You need to a chose a cloud service \"best\" for\nyour project. Following checklist will help:\n\uf0b7 Is your desired region supported?\n\uf0b7 Cost for the service and your budget\nK.NIKHILA Page 33\n\nUNIT -1\n\uf0b7 For an outsourcing company, Customer/Client Preference of service provider needs to be\nfactored in\n\uf0b7 Cost involved in training employees on the Cloud Service Platform\n\uf0b7 Customer support\n\uf0b7 The provider should have a successful track record of stability/uptime/reliability\n\uf0b7 Reviews of the company\nHere is a list of Top 21 Cloud Service Providers for Quick Reference\n\uf0b7 Amazon Web Services Alibaba Cloud\n\uf0b7 Microsoft Azure Google Cloud Platform\n\uf0b7 VMware Rackspace\n\uf0b7 Salesforce Oracle Cloud\n\uf0b7 Verizon Cloud Navisite\n\uf0b7 IBM Cloud OpenNebula\n\uf0b7 Pivotal DigtialOceanCloudSigma Dell Cloud\n\uf0b7 LiquidWeb LimeStone\nMassiveGridQuadranet Kamatera\nEucalyptus\n\u2022 Eucalyptus is an acronym for Elastic Utility Computing Architecture for Linking\nYour Programs To Useful Systems.\n\u2022 Eucalyptus is a paid and open-source computer software for building Amazon Web\nServices (AWS)-compatible private and hybrid cloud computing environments, originally\ndeveloped by the company Eucalyptus Systems.\n\u2022 Eucalyptus enables pooling compute, storage, and network resources that can be\ndynamically scaled up or down as application workloads change\nK.NIKHILA Page 34\n\nUNIT -1\nEucalyptus has six components:\n1.The Cloud Controller (CLC) is a Java program that offers EC2-compatible interfaces,\nas well as a web interface to the outside world.\n\u2022 In addition to handling incoming requests, the CLC acts as the administrative interface\nfor cloud management and performs high-level resource scheduling and system\naccounting.\n\u2022 The CLC accepts user API requests from command-line interfaces like euca2ools or\nGUI-based tools like the Eucalyptus User Console and manages the underlying compute,\nstorage, and network resources.\n\u2022 Only one CLC can exist per cloud and it handles authentication, accounting, reporting,\nand quota management.\n2.Walrus, also written in Java, is the Eucalyptus equivalent to AWS Simple Storage\nService (S3).\nK.NIKHILA Page 35\n\nUNIT -1\n\u2022 Walrus offers persistent storage to all of the virtual machines in the Eucalyptus cloud and\ncan be used as a simple HTTP put/get storage as a service solution.\n\u2022 There are no data type restrictions for Walrus, and it can contain images (i.e., the building\nblocks used to launch virtual machines), volume snapshots (i.e., point-in-time copies),\nand application data. Only one Walrus can exist per cloud.\n3.The Cluster Controller (CC) is written in C and acts as the front end for a cluster\nwithin a Eucalyptus cloud and communicates with the Storage Controller and Node\nController.\n\u2022 It manages instance (i.e., virtual machines) execution and Service Level Agreements\n(SLAs) per cluster.\n4.The Storage Controller (SC) is written in Java and is the Eucalyptus equivalent to\nAWS EBS. It communicates with the Cluster Controller and Node Controller and\nmanages Eucalyptus block volumes and snapshots to the instances within its specific\ncluster.\n\u2022 If an instance requires writing persistent data to memory outside of the cluster, it would\nneed to write to Walrus, which is available to any instance in any cluster.\n5.The Node Controller (NC) is written in C and hosts the virtual machine instances and\nmanages the virtual network endpoints.\n\u2022 It downloads and caches images from Walrus as well as creates and caches instances.\n\u2022 While there is no theoretical limit to the number of Node Controllers per cluster,\nperformance limits do exist.\n6.The VMware Broker is an optional component that provides an AWS-compatible\ninterface for VMware environments and physically runs on the Cluster Controller.\n\u2022 The VMware Broker overlays existing ESX/ESXi hosts and transforms Eucalyptus\nMachine Images (EMIs) to VMware virtual disks.\n\u2022 The VMware Broker mediates interactions between the Cluster Controller and VMware\nand can connect directly to either ESX/ESXi hosts or to vCenter Server.\nNimbus\n\u2022 Nimbus is a set of open source tools that together provide an \"Infrastructure-as-a-\nService\" (IaaS) cloud computing solution.\n\u2022 Mission is to evolve the infrastructure with emphasis on the needs of science, but many\nnon-scientific use cases are supported as well.\nK.NIKHILA Page 36\n\nUNIT -1\n\u2022 Nimbus allows a client to lease remote resources by deploying virtual machines (VMs)\non those resources and configuring them to represent an environment desired by the user.\n\u2022 It was formerly known as the \"Virtual Workspace Service\" (VWS) but the \"workspace\nservice\" is technically just one the components in the software .\n\u2022 Nimbus is a toolkit that, once installed on a cluster, provides an infrastructure as a service\ncloud to its client via WSRF-based or Amazon EC2 WSDL web service APIs.\n\u2022 Nimbus is free and open-source software, subject to the requirements of the Apache\nLicense, version 2.\n\u2022 Nimbus supports both the hypervisors Xen and KVM and virtual machine schedulers\nPortable Batch System and Oracle Grid Engine.\n\u2022 It allows deployment of self-configured virtual clusters via contextualization.\n\u2022 It is configurable with respect to scheduling, networking leases, and usage accounting.\n\u2022 Nimbus is comprised of two products:\nNimbus Infrastructure\nNimbus Platform\n\u2022 Nimbus Infrastructure is an open source EC2/S3-compatible Infrastructure-as-a-\nService implementation specifically targeting features of interest to the scientific\ncommunity such as support for proxy credentials, batch schedulers, best-effort allocations\nand others.\n\u2022 Nimbus Platform is an integrated set of tools, operating in a multi-cloud environment,\nthat deliver the power and versatility of infrastructure clouds to scientific users. Nimbus\nPlatform allows you to reliably deploy, scale, and manage cloud resources.\nSystem Architecture & Design\nK.NIKHILA Page 37\n\nUNIT -1\n\u2022 The design of nimbus which consists of a number of components based on the web\nservice technology.\n1. Workspace service\nAllows clients to manage and administer VMs by providing to two interfaces:\n\u2022 A)One interface is based on the web service is resource framework (WSRF)\n\u2022 B)The other is based on EC2 WSDL\n2. Workspace resource manager\nimplements VM instance creation on a site management.\n3. Workspace pilot\n\u2022 Provides virtualization with significant changes to the site configurations.\n4. workspace control\n\u2022 Implements VM instance management such as start, stop and pause VM. It also provides\nimage management and set up networks and provides IP assignment.\n5.context Broker\n\u2022 Allows clients coordinate large virtual cluster launches automatically and repeatedly.\nK.NIKHILA Page 38\n\nUNIT -1\n6. Workspace client\n\u2022 A complex client that provides full access to the workspace service functionality.\n7. Cloud client\n\u2022 A simpler client providing access to selected functionalities in the workspace service.\n8. Storage service\n\u2022 cumulus is a web service providing users with storage capabilities to store images and\nworks in conjunction with GridFTP.\nOpen Nebula\n\u2022 Open Nebula- is an open source cloud computing platform for managing heterogeneous\ndistributed data centre infrastructures.\n\u2022 Manages a data centre\u2019s virtual infrastructure to build private,public and hybrid\nimplementations of Iaas.\n\u2022 Two primary uses of open nebula platform are:\ndata center virtualization\n\u2022 Many of our users use OpenNebula to manage data center virtualization, consolidate\nservers, and integrate existing IT assets for computing, storage, and networking.\n\u2022 In this deployment model, OpenNebula directly integrates with hypervisors (like KVM,\nXen or VMware ESX) and has complete control over virtual and physical resources,\nproviding advanced features for capacity management, resource optimization, high\navailability and business continuity.\n\u2022 Some of these users also enjoy OpenNebula\u2019s cloud management and provisioning\nfeatures when they additional want to federate data centers, implement cloud bursting, or\noffer self-service portals for users.\nCloud infrastructure solutions\n\u2022 We also have users that use OpenNebula to provide a multitenant, cloud-like\nprovisioning layer on top of an existing infrastructure management solution (like\nVMware vCenter).\n\u2022 These users are looking for provisioning, elasticity and multi-tenancy cloud features like\nvirtual data centers provisioning, datacenter federation or hybrid cloud computing to\nK.NIKHILA Page 39\n\nUNIT -1\nconnect in-house infrastructures with public clouds, while the infrastructure is managed\nby already familiar tools for infrastructure management and operation\nImage Repository: Any storage medium for the VM images (usually a high performing SAN).\nCluster Storage : OpenNebula supports multiple back-ends (e.g. LVM for fast cloning)\nVM Directory: The home of the VM in the cluster node\n\uf0b7 Stores checkpoints, description files and VM disks\n\uf0b7 Actual operations over the VM directory depends on the storage medium\n\uf0b7 Should be shared for live-migrations\n\uf0b7 You can go on without a shared FS and use the SSH back-e\uf06c\nK.NIKHILA Page 40\n\nUNIT -1\nMaster node: A single gateway or front-end machine, sometimes also called the master node, is\nresponsible for queuing, scheduling and submitting jobs to the machines in the cluster. It runs\nseveral other OpenNebula services mentioned below:\n\uf0b7 Provides an interface to the user to submit virtual machines and monitor their status.\n\uf0b7 Manages and monitors all virtual machines running on different nodes in the cluster.\n\uf0b7 It hosts the virtual machine repository and also runs a transfer service to manage the\ntransfer of virtual machine images to the concerned worker nodes.\n\uf0b7 Provides an easy-to-use mechanism to set up virtual networks in the cloud.\n\uf0b7 Finally, the front-end allows you to add new machines to your cluster.\nWorker node: The other machines in the cluster, known as \u2018worker nodes\u2019, provide raw\ncomputing power for processing the jobs submitted to the cluster. The worker nodes in an\nK.NIKHILA Page 41\n\nUNIT -1\nOpenNebula cluster are machines that deploy a virtualisation hypervisor, such as VMware, Xen\nor KVM.\nCloudSim\n\u2022 CloudSim is a framework for modeling and simulation of cloud computing\ninfrastructures and services.\n\u2022 Originally built primarily at the Cloud Computing and Distributed Systems (CLOUDS)\nLaboratory, The University of Melbourne, Australia, CloudSim has become one of the\nmost popular open source cloud simulators in the research and academia.\n\u2022 CloudSim is completely written in Java.\n\u2022 By using CloudSim, developers can focus on specific systems design issues that they\nwant to investigate, without getting concerned about details related to cloud-based\ninfrastructures and services.\n\u2022 CloudSim is a simulation tool that allows cloud developers to test the performance of\ntheir provisioning policies in a repeatable and controllable environment, free of cost.\n\u2022 It helps tune the bottlenecks before real-world deployment.\n\u2022 It is a simulator; hence, it doesn\u2019t run any actual software.\n\u2022 It can be defined as \u2018running a model of an environment in a model of hardware\u2019, where\ntechnology-specific details are abstracted.\n\u2022 CloudSim is a library for the simulation of cloud scenarios.\n\u2022 It provides essential classes for describing data centres, computational resources, virtual\nmachines, applications, users, and policies for the management of various parts of the\nsystem such as scheduling and provisioning.\n\u2022 It can be used as a building block for a simulated cloud environment and can add new\npolicies for scheduling, load balancing and new scenarios.\n\u2022 It is flexible enough to be used as a library that allows you to add a desired scenario by\nwriting a Java program.\nFeatures of Cloudsim\nK.NIKHILA Page 42\n\nUNIT -1\nArchitecture of CloudSim\nK.NIKHILA Page 43\n\nUNIT -1\n\u2022 The user code :layer exposes basic entities such as the number of machines, their\nspecifications, etc, as well as applications, VMs, number of users, application types and\nscheduling policies.\n\u2022 The User Code layer is a custom layer where the user writes their own code to redefine\nthe characteristics of the stimulating environment as per their new research findings.\n\u2022 Network Layer: This layer of CloudSim has responsibility to make communication\npossible between different layers. This layer also identifies how resources in cloud\nenvironment are places and managed.\n\u2022 Cloud Resources: This layer includes different main resources like datacenters, cloud\ncoordinator (ensures that different resources of the cloud can work in a collaborative\nway) in the cloud environment\n\u2022 Cloud Services: This layer includes different service provided to the user of cloud\nservices. The various services of clouds include Information as a Service (IaaS), Platform\nas a Service (PaaS), and Software as a Service (SaaS)\nK.NIKHILA Page 44\n\nUNIT -1\n\u2022 User Interface: This layer provides the interaction between user and the simulator.\n\u2022 The CloudSim Core simulation engine provides support for modeling and simulation of\nvirtualized Cloud-based data center environments including queuing and processing of\nevents, creation of cloud system entities (like data center, host, virtual machines, brokers,\nservices, etc.) communication between components and management of the simulation\nclock.\nK.NIKHILA Page 45\n\n1.Cloud computing services\n1.1 Infrastructure as a service - IaaS\nAWS supports everything you need to build and run Windows applications including Active\nDirectory, .NET, System Center, Microsoft SQL Server, Visual Studio, and the first and only\nfully managed native-Windows file system available in the cloud with FSx for Windows File\nServer.\nThe AWS advantage for Windows over the next largest cloud provider\n2x More Windows Server instances\n2x more regions with multiple availability zones\n7x fewer downtime hours in 2018*\n2x higher performance for SQL Server on Windows\n5x more services offering encryption\nAWS offers the best cloud for Windows, and it is the right cloud platform for running Windows-\nbased applications\nWindows on Amazon EC2 enables you to increase or decrease capacity within minutes\ni. Broader and Deeper Functionality\nii. Greater Reliability\niii. More Security Capabilities\niv. Faster Performance\nv. Lower Costs\nvi. More Migration Experience\nPopular AWS services for Windows workloads\ni. SQL Server on Amazon EC2\nii. Amazon Relational Database Service\niii. Amazon FSx for Window File Server\niv. AWS Directory Service\nv. AWS License Manager\n\nService-level agreement (SLA)\nA service-level agreement (SLA) is a contract between a service provider and its internal or\nexternal customers that documents what services the provider will furnish and defines the service\nstandards the provider is obligated to meet.\n\nUNIT III\nCLOUD STORAGE\n3.1.1 Overview\nThe Basics\nCloud storage is nothing but storing our data with a cloud service provider rather\nthan on a local system, as with other cloud services, we can access the data stored on the cloud\nvia an Internet link. Cloud storage has a number of advantages over traditional data storage. If\nwe store our data on a cloud, we can get at it from any location that has Internet access.\nAt the most rudimentary level, a cloud storage system just needs one data server connected to\nthe Internet. A subscriber copies files to the server over the Internet, which then records the\ndata. When a client wants to retrieve the data, he or she accesses the data server with a web-\nbased interface, and the server then either sends the files back to the client or allows the client\nto access and manipulate the data itself.\nCloud storage systems utilize dozens or hundreds of data servers. Because servers require\nmaintenance or repair, it is necessary to store the saved data on multiple machines, providing\nredundancy. Without that redundancy, cloud storage systems couldn\u2019t assure clients that they\ncould access their information at any given time. Most systems store the same data on servers\nusing different power supplies. That way, clients can still access their data even if a power\nK.NIKHILA Page 1\n\nUNIT III\nsupply fails.\nb.Storage as a Service\nThe term Storage as a Service (another Software as a Service, or SaaS, acronym) means that a\nthird-party provider rents space on their storage to end users who lack the budget or capital\nbudget to pay for it on their own. It is also ideal when technical personnel are not available or\nhave inadequate knowledge to implement and maintain that storage infrastructure. Storage\nservice providers are nothing new, but given the complexity of current backup,\nreplication, and disaster recovery needs, the service has become popular, especially among\nsmall and medium-sized businesses. Storage is rented from the provider using a cost-per-\ngigabyte-stored or cost-per-data-transferred model. The end user doesn\u2019t have to pay for\ninfrastructure; they simply pay for how much they transfer and save on the provider\u2019s\nservers.\nA customer uses client software to specify the backup set and then transfers data across a\nWAN. When data loss occurs, the customer can retrieve the lost data from the service provider.\nc.Providers\nThey are hundreds of cloud storage providers on the Web, and more seem to be added each\nday. Not only are there general-purpose storage providers, but there are some that are very\nspecialized in what they store.\n\uf0fc Google Docs allows users to upload documents, spreadsheets, and presentations to\nK.NIKHILA Page 2\n\nUNIT III\nGoogle\u2019s data servers. Those files can then be edited using a Google application.\n\uf0fc Web email providers like Gmail, Hotmail, and Yahoo! Mail store email messages on their\nown servers. Users can access their email from computers and other devices connected to the\nInternet.\n\uf0fc Flickr and Picasa host millions of digital photographs. Users can create their own online\nphoto albums.\n\uf0fc YouTube hosts millions of user-uploaded video files.\n\uf0fc Hostmonster and GoDaddy store files and data for many client web sites.\n\uf0fc Facebook and MySpace are social networking sites and allow members to post\npictures and other content. That content is stored on the company\u2019s servers.\n\uf0fc MediaMax and Strongspace offer storage space for any kind of digital data.\nd. Security:\nTo secure data, most systems use a combination of techniques:\ni. Encryption A complex algorithm is used to encode information. To decode the encrypted\nfiles, a user needs the encryption key. While it\u2019s possible to crack encrypted\ninformation, it\u2019s very difficult and most hackers don\u2019t have access to the amount of\ncomputer processing power they would need to crack the code.\nii. Authentication processes this requires a user to create a name and password.\niii. Authorization practices The client lists the people who are authorized to access\ninformation stored on the cloud system. Many corporations have multiple levels of\nauthorization. For example, a front-line employee might have limited access to data\nstored on the cloud and the head of the IT department might have complete and free\naccess to everything.\nK.NIKHILA Page 3\n\nUNIT III\ne. Reliability\nMost cloud storage providers try to address the reliability concern through redundancy, but the\npossibility still exists that the system could crash and leave clients with no way to access their\nsaved data.\nAdvantages\n\uf0b7 Cloud storage is becoming an increasingly attractive solution for organizations. That\u2019s\nbecause with cloud storage, data resides on the Web, located across storage systems\nrather than at a designated corporate hosting site. Cloud storage providers balance\nserver loads and move data among various datacenters, ensuring that information is\nstored close to where it is used.\n\uf0b7 Storing data on the cloud is advantageous, because it allows us to protect our data\nincase there\u2019s a disaster. we may have backup files of our critical information, but if\nthere is a fire or a hurricane wipes out our organization, having the backups stored\nlocally doesn\u2019t help.\n\uf0b7 Amazon S3 is the best-known storage solution, but other vendors might be better for\nlarge enterprises. For instance, those who offer service level agreements and direct\naccess to customer support are critical for a business moving storage to a service\nprovider.\nK.NIKHILA Page 4\n\nUNIT III\n\uf0b7 A lot of companies take the \u201cappetizer\u201d approach, testing one or two services to see\nhow well they mesh with their existing IT systems. It\u2019s important to make sure the\nservices will provide what we need before we commit too much to the cloud.\n3.1.2 Cloud Storage Providers\nAmazon and Nirvanix are the current industry top storage providers.\na. Amazon Simple Storage Service (S3)\n\uf0b7 The best-known cloud storage service is Amazon\u2019s Simple Storage Service (S3), which\nlaunched in 2006.\n\uf0b7 Amazon S3 is designed to make web-scale computing easier for developers. Amazon S3\nprovides a simple web services interface that can be used to store and retrieve any\namount of data, at any time, from anywhere on the Web. It gives any developer access\nto the same highly scalable data storage infrastructure that Amazon uses to run its own\nK.NIKHILA Page 5\n\nUNIT III\nglobal network of websites. The service aims to maximize benefits of scale and to pass\nthose benefits on to developers.\nAmazon S3 is intentionally built with a minimal feature set that includes the following\nfunctionality:\n\uf0fc Write, read, and delete objects containing from 1 byte to 5 gigabytes of data\neach. The number of objects that can be stored is unlimited.\n\uf0fc Each object is stored and retrieved via a unique developer-assigned key.\n\uf0fc Objects can be made private or public, and rights can be assigned to specific users.\n\uf0fc Uses standards-based REST and SOAP interfaces designed to work with any\nInternet- development toolkit.\nDesign Requirements\nAmazon built S3 to fulfill the following design requirements:\n\uf0fc Scalable Amazon S3 can scale in terms of storage, request rate, and users to\nsupport an unlimited number of web-scale applications.\n\uf0fc Reliable Store data durably, with 99.99 percent availability. Amazon says it does not\nallow any downtime.\n\uf0fc Fast Amazon S3 was designed to be fast enough to support high-performance\napplications. Server-side latency must be insignificant relative to Internet latency. Any\nperformance bottlenecks can be fixed by simply adding nodes to the system.\n\uf0fc Inexpensive Amazon S3 is built from inexpensive commodity hardware components.\nAs a result, frequent node failure is the norm and must not affect the overall system. It\nmust be hardware-agnostic, so that savings can be captured as Amazon continues to drive\ndown infrastructure costs.\n\uf0fc Simple Building highly scalable, reliable, fast, and inexpensive storage is difficult. Doing\nso in a way that makes it easy to use for any application anywhere is more difficult. Amazon S3\nmust do both.\nDesign Principles\nAmazon used the following principles of distributed system design to meet Amazon S3\nrequirements:\nK.NIKHILA Page 6\n\nUNIT III\n\uf0fc Decentralization It uses fully decentralized techniques to remove scaling bottlenecks\nand single points of failure.\n\uf0fc Autonomy The system is designed such that individual components can make decisions\nbased on local information.\n\uf0fc Local responsibility Each individual component is responsible for achieving its\nconsistency; this is never the burden of its peers.\n\uf0fc Controlled concurrency Operations are designed such that no or limited concurrency\ncontrol is required.\n\uf0fc Failure toleration The system considers the failure of components to be a normal\nmode of operation and continues operation with no or minimal interruption.\n\uf0fc Controlled parallelism Abstractions used in the system are of such granularity that\nparallelism can be used to improve performance and robustness of recovery or the introduction\nof new nodes.\n\uf0fc Small, well-understood building blocks Do not try to provide a single service that\ndoes everything for everyone, but instead build small components that can be used as\nbuilding blocks for other services.\n\uf0fc Symmetry Nodes in the system are identical in terms of functionality, and require\nno or minimal node-specific configuration to function.\n\uf0fc Simplicity The system should be made as simple as possible, but no simpler.\nHow S3 Works\nS3 stores arbitrary objects at up to 5GB in size, and each is accompanied by up to 2KB of\nmetadata. Objects are organized by buckets. Each bucket is owned by an AWS account and the\nbuckets are identified by a unique, user-assigned key.\nK.NIKHILA Page 7\n\nUNIT III\nBuckets and objects are created, listed, and retrieved using either a REST-style or SOAP\ninterface. Objects can also be retrieved using the HTTP GET interface or via BitTorrent.\nAn access control list restricts who can access the data in each bucket. Bucket names and keys\nare formulated so that they can be accessed using HTTP. Requests are authorized using an\naccess control list associated with each bucket and object, for instance:\nb Nirvanix\nNirvanix uses custom-developed software and file system technologies running on Intel storage\nservers at six locations on both coasts of the United States. They continue to grow, and expect\nto add dozens more server locations. SDN Features Nirvanix Storage Delivery Network (SDN)\nturns a standard 1U server into an infinite capacity network attached storage (NAS) file\naccessible by popular applications and immediately integrates into an organization\u2019s existing\narchive and backup processes.\nNirvanix has built a global cluster of storage nodes collectively referred to as the Storage\nDelivery Network (SDN), powered by the Nirvanix Internet Media File System (IMFS). The SDN\nintelligently stores, delivers, and processes storage requests in the best network location,\nproviding the best user experience in the marketplace.\nBenefits of CloudNAS: The benefits of cloud network attached storage (CloudNAS) include\nK.NIKHILA Page 8\n\nUNIT III\n\uf0fc Cost savings of 80\u201390 percent over managing traditional storage solutions\n\uf0fc Elimination of large capital expenditures while enabling 100 percent storage utilization\n\uf0fc Encrypted offsite storage that integrates into existing archive and backup processes\n\uf0fc Built-in data disaster recovery and automated data replication on up to three\ngeographically dispersed storage nodes for a 100% SLA\n\uf0fc Immediate availability to data in seconds, versus hours or days on offline tape.\nc.Google Bigtable Datastore:\n\uf0b7 Datastore In cloud computing, it\u2019s important to have a database that is capable of\nhandling numerous users on an on-demand basis. To serve that market, Google\nintroduced its Bigtable. Google started working on it in 2004 and finally went public with\nit in April 2008. Bigtable was developed with very high speed, flexibility,\nand extremely high scalability in mind. A Bigtable database can be petabytes in\nsize and span thousands of distributed servers. Bigtable is available to developers as\npart of the Google App Engine, their cloud computing platform.\n\uf0b7 Google describes Bigtable as a fast and extremely scalable DBMS. This allows Bigtable to\nscale across thousands of commodity servers that can collectively store petabytes of\ndata. Each table in Bigtable is a multidimensional sparse map. That is, the table is made\nup of rows and columns, and each cell has a timestamp. Multiple versions of a cell can\nexist, each with a different timestamp. With this stamping, we can select certain\nversions of a web page, or delete cells that are older than a given date and time.\nK.NIKHILA Page 9\n\nUNIT III\nd. MobileMe:\n\uf0b7 it is Apple\u2019s solution that delivers push email, push contacts, and push calendars from\nthe MobileMe service in the cloud to native applications on iPhone, iPod touch, Macs,\nand PCs.\n\uf0b7 It provides a suite of ad-free web applications that deliver a desktop like experience\nthrough any browser.\ne. Live Mesh:\n\uf0b7 It is Microsoft\u2019s \u201csoftware plus services\u201d platform and experience that enables PCs and\nother devices to be aware of each other through internet,enabling individuals and\norganizations to manage ,access and share their files and applications on the web.\ncomponents:\n\uf0b7 A platform that defines and models a user\u2019s digital relationships among devices,\ndata, applications, and people\u2014made available to developers through an open data\nmodel and protocols.\n\uf0b7 A cloud service providing an implementation of the platform hosted in Microsoft\nK.NIKHILA Page 10\n\nUNIT III\ndatacenters.\n\uf0b7 Software, a client implementation of the platform that enables local applications to\nrun offline and interact seamlessly with the cloud.\n\uf0b7 A platform experience that exposes the key benefits of the platform for bringing together\na user\u2019s devices, files and applications, and social graph, with news feeds across all of\nthese.\nStandards\n\uf0b7 Standards make the World Wide Web go around, and by extension, they are important to\ncloud computing. Standards are what make it possible to connect to the cloud and what\nmake it possible to develop and deliver content.\n3.2.1 Applications\nA cloud application is the software architecture that the cloud uses to eliminate the need to\ninstall and run on the client computer. There are many applications that can run, but there\nneeds to be a standard way to connect between the client and the cloud.\na. Communication:\nHTTP\nTo get a web page from our cloud provider, we will likely be using the Hypertext Transfer\nProtocol (HTTP) as the computing mechanism to transfer data between the cloud and our\norganization. HTTP is a stateless protocol. This is beneficial because hosts do not need to retain\ninformation about users between requests, but this forces web developers to use alternative\nmethods for maintaining users\u2019 states. HTTP is the language that the cloud and our computers\nuse to communicate.\nXMPP\nThe Extensible Messaging and Presence Protocol (XMPP) is being talked about as the next\nbig thing for cloud computing.\nK.NIKHILA Page 11\n\nUNIT III\nThe Problem with Polling When we wanted to sync services between two servers, the most\ncommon means was to have the client ping the host at regular intervals. This is known as\npolling. This is generally how we check our email. Every so often, we ping our email server to\nsee if we got any new messages. It\u2019s also how the APIs for most web services work.\nb. security\n\uf0b7 SSL is the standard security technology for establishing an encrypted link between a web\nserver and browser. This ensures that data passed between the browser and the web\nserver stays private. To create an SSL connection on a web server requires an SSL\ncertificate. When our cloud provider starts an SSL session, they are prompted to\ncomplete a number of questions about the identity of their company and web site. The\ncloud provider\u2019s computers then generate two cryptographic keys\u2014a public key and a\nprivate key.\nK.NIKHILA Page 12\n\nUNIT III\n3.2.2 Client\na.HTML\n\uf0b7 HTML is to improve its usability and functionality.W3C is the organization that is charged\nwith designing and maintaining the language. When you click on a link in a web page,\nyou are accessing HTML code in the form of a hyperlink, which then takes you to another\npage.\nHow HTML works??\ni. HTML is a series of short codes typed into a text file called TAGS which is created by web\npage design software.\nii. This text is saved as an HTML file and viewed through a browser.\niii. The browser reads the file and translates the text into the form the author wanted you\nto see.\n\uf0b7 Writing HTML can be done using a number of methods, with either a simple text editor\nor a powerful graphical editor.\n\uf0b7 Tags are seen like normal text but in <angle brackets>.tags is what allows things like\ntables and images to appear in a web page.\n\uf0b7 Different tags perform different functions. Here tags cannot be seen through browser\nbut affects how the browser behaves.\n\uf0b7\nK.NIKHILA Page 13\n\nUNIT III\nb.DHTML: There are four parts to DHTML:\nDOM: (Document Object Model) allows you to access web page and makes changes with\nDHTML.DOM specifies every part of a web page and provides consistent naming\nconventions, allowing accessing your web pages and changing their properties.\nScripts: common scripting language in DHTML. Are java scripts and ActiveX. Scripts are\nused to control the objects specified in the DOM.\nCSS: (Cascading style sheets) are used to control the look and feel of web page, where\nstyle sheets list the color and font s of text, the background colors and images, and the\nplacement of objects on the page. Using scripting and DOM you can change the style of\nvarious elements.\nXHTML: nothing unique about XHTML but it is important because there are more things\nworking from it than just the browser.\nDHTML features:\nFour main features are:\ni. Changing the tags and properties\nii. Real-time positioning\niii. Dynamic fonts\niv. Data binding\n3.2.3 Infrastructure\nInfrastructure is a way to deliver virtualization to our cloud computing solution.\na.Virtualization: Whenever something new happens in the world of computing, competitors\nduke it out to have their implementation be the standard. Virtualization is somewhat different,\nand major players worked together to develop a standard.\nK.NIKHILA Page 14\n\nUNIT III\nVMware, AMD, BEA Systems, BMC Software, Broadcom, Cisco, Computer Associates\nInternational, Dell, Emulex, HP, IBM, Intel, Mellanox, Novell, QLogic, and Red Hat all worked\ntogether to advance open virtualization standards. VMware says that it will provide its partners\nwith access to VMware ESX Server source code and interfaces under a new program called\nVMware Community Source. This program is designed to help partners influence the direction\nof VMware ESX Server through a collaborative development model and shared governance\nprocess.\nThese initiatives are intended to benefit end users by :\ni. Expanding virtualization solutions the availability of open-standard virtualization interfaces\nand the collaborative nature of VMware Community Source are intended to accelerate the\navailability of new virtualization solutions.\nii. Expanded interoperability and supportability Standard interfaces for hypervisors are expected\nto enable interoperability for customers with heterogeneous virtualized environments.\niii. Accelerated availability of new virtualization-aware technologies Vendors across the\ntechnology stack can optimize existing technologies and introduce new technologies for\nrunning in virtual environments.\nOpen Hypervisor Standards\nHypervisors are the foundational component of virtual infrastructure and enable\ncomputer system partitioning. An open-standard hypervisor framework can benefit customers\nby enabling innovation across an ecosystem of interoperable virtualization vendors and\nsolutions.\nK.NIKHILA Page 15\n\nUNIT III\nVMware contributed an existing framework of interfaces, called Virtual Machine Hypervisor\nInterfaces (VMHI), based on its virtualization products to facilitate the development of these\nstandards in an industry-neutral manner.\nCommunity Source\nThe Community Source program provides industry partners with an opportunity to access\nVMware ESX Server source code under a royalty-free license. Partners can contribute shared\ncode or create binary modules to spur and extend interoperable and integrated virtualization\nsolutions. The idea is to combine the best of both the traditional commercial and open-source\ndevelopment models. Community members can participate and influence the governance of\nVMware ESX Server through an architecture board.\nb.OVF\nAs the result of VMware and its industry partners\u2019 efforts, a standard has already been\ndeveloped called the Open Virtualization Format (OVF). OVF describes how virtual appliances\ncan be packaged in a vendor-neutral format to be run on any hypervisor. It is a platform-\nindependent, extensible, and open specification for the packaging and distribution of virtual\nappliances composed of one or more virtual machines.\nVMware developed a standard with these features:\n\uf0b7 Optimized for distribution\n\uf0fc Enables the portability and distribution of virtual appliances\n\uf0fc Supports industry-standard content verification and integrity checking\n\uf0fc Provides a basic scheme for the management of software licensing\n\uf0b7 A simple, automated user experience\n\uf0fc Enables a robust and user-friendly approach to streamlining the installation process\n\uf0fc Validates the entire package and confidently determines whether each virtual machine should\nbe installed\n\uf0fc Verifies compatibility with the local virtual hardware\n\uf0b7 Portable virtual machine packaging\n\uf0fc Enables platform-specific enhancements to be captured\nK.NIKHILA Page 16\n\nUNIT III\n\uf0fc Supports the full range of virtual hard disk formats used for virtual machines today, and is\nextensible to deal with future formats that are developed\n\uf0fc Captures virtual machine properties concisely and accurately\n\uf0b7 Vendor and platform independent\n\uf0fc Does not rely on the use of a specific host platform, virtualization platform, or guest\noperating system\n\uf0b7 Extensible\n\uf0fc Designed to be extended as the industry moves forward with virtual appliance technology\n\uf0b7 Localizable\n\uf0fc Supports user-visible descriptions in multiple locales\n\uf0fc Supports localization of the interactive processes during installation of an appliance\n\uf0fc Allows a single packaged appliance to serve multiple market opportunities\n3.2.4 Service\n\uf0b7 A web service, as defined by the World Wide Web Consortium (W3C), \u201cis a software system\ndesigned to support interoperable machine-to-machine interaction over a network\u201d that may\nbe accessed by other cloud computing components. Web services are often web API\u2019s that can\nbe accessed over a network, like the Internet, and executed on a remote system that hosts the\nrequested services.\na.Data\nData can be stirred and served up with a number of mechanisms; two of the most popular are\nJSON and XML.\nJSON\nJSON is short for JavaScript Object Notation and is a lightweight computer data interchange\nformat. It is used for transmitting structured data over a network connection in a process called\nserialization. It is often used as an alternative to XML.\nJSON Basics JSON is based on a subset of JavaScript and is normally used with that language.\nHowever, JSON is considered to be a language-independent format, and code for parsing and\ngenerating JSON data is available for several programming languages. This makes it a good\nreplacement for XML when JavaScript is involved with the exchange of data, like AJAX.\nK.NIKHILA Page 17\n\nUNIT III\nXML vs. JSON:\nJSON should be used instead of XML when JavaScript is sending or receiving data. The reason\nfor this is that when we use XML in JavaScript, we have to write scripts or use libraries to handle\nthe DOM objects to extract the data our need. However, in JSON, the object is already an\nobject, so no extra work needs to be done.\nXML\nExtensible Markup Language (XML) is a standard, self-describing way of encoding text and data\nso that content can be accessed with very little human interaction and exchanged across a wide\nvariety of hardware, operating systems, and applications. XML provides a standardized way to\nrepresent text and data in a format that can be used across platforms. It can also be used with a\nwide range of development tools and utilities.\nHTML vs XML\n\uf0fc Separation of form and content HTML uses tags to define the appearance of text, while\nXML tags define the structure and the content of the data. Individual applications will be\nspecified by the application or associated style sheet.\n\uf0fc XML is extensible Tags can be defined by the developer for specific application, while\nHTML\u2019s tags are defined by W3C.\nK.NIKHILA Page 18\n\nUNIT III\nBenefits of XML include:\ni. Self-describing data XML does not require relational schemata, file description tables, external\ndata type definitions, and so forth. Also, while HTML only ensures the correct presentation of\nthe data, XML also guarantees that the data is usable.\nii. Database integration XML documents can contain any type of data\u2014from text and numbers to\nmultimedia objects to active formats like Java.\niii. No reprogramming if modifications are made Documents and web sites can be changed with\nXSL Style Sheets, without having to reprogram the data.\niv. One-server view of data XML is exceptionally ideal for cloud computing, because data spread\nacross multiple servers looks as if it is stored on one server.\nv. Open and extensible XML\u2019s structure allows us to add other elements if we need them. We can\neasily adapt our system as our business changes.\nvi. Future-proof The W3C has endorsed XML as an industry standard, and it is supported by all\nleading software providers. It\u2019s already become industry standard in fields like healthcare.\nvii. Contains machine-readable context information Tags, attributes, and element structure\nprovide the context for interpreting the meaning of content, which opens up possibilities for\ndevelopment.\n\uf0fc Content vs. presentation XML tags describe the meaning of the object, not its presentation.\nThat is, XML describes the look and feel of a document, and the application presents it as\ndescribed.\nb.Web Services\nWeb services describe how data is transferred from the cloud to the client.\nREST\nRepresentational state transfer (REST) is a way of getting information content from a web site\nby reading a designated web page that contains an XML file that describes and includes the\ndesired content.\nFor instance, REST could be used by our cloud provider to provide updated subscription\ninformation. Every so often, the provider could prepare a web page that includes content and\nXML statements that are described in the code. Subscribers only need to know the uniform\nK.NIKHILA Page 19\n\nUNIT III\nresource locator (URL) for the page where the XML file is located, read it with a web browser,\nunderstand the content using XML information, and display it appropriately.\nREST is similar in function to the Simple Object Access Protocol (SOAP), but is easier to use.\nSOAP requires writing or using a data server program and a client program (to request the\ndata). However, SOAP offers more capability. For instance, if we were to provide syndicated\ncontent from our cloud to subscribing web sites, those subscribers might need to use SOAP,\nwhich allows greater program interaction between the client and the server.\nBenefits REST offers the following benefits:\n\uf0fc It gives better response time and reduced server load due to its support for the caching of\nrepresentations.\n\uf0fc Server scalability is improved by reducing the need to maintain session state.\n\uf0fc A single browser can access any application and any resource, so less client-side software\nneeds to be written.\n\uf0fc A separate resource discovery mechanism is not needed, due to the use of hyperlinks in\nrepresentations.\nK.NIKHILA Page 20\n\nUNIT III\n\uf0fc Better long-term compatibility and resolvability characteristics exist than\nin RPC. This is due to:\n\u2022 The ability of documents, like HTML, to evolve with both forward- and backward-\ncompatibility.\n\u2022 Resources can add support for new content types as they are defined, without\neliminating support for older content types.\nSOAP\n\uf0b7 Simple Object Access Protocol (SOAP) is a way for a program running in one kind of operating\nsystem (such as Windows Vista) to communicate with a program in the same or another kind of\nan operating system (such as Linux) by using HTTP and XML as the tools to exchange\ninformation.\n\uf0b7 Procedure Calls Often, remote procedure calls (RPC) are used between objects like DCOM or\nCOBRA, but HTTP was not designed for this use. RPC is a compatibility problem, because\nfirewall and proxy servers will block this type of traffic. Because web protocols already are\ninstalled and available for use by the major operating systems, HTTP and XML provide an easy\nsolution to the problem of how programs running under different operating systems in a\nnetwork can communicate with each other.\n\uf0b7 SOAP describes exactly how to encode an HTTP header and an XML file so that a program on\none computer can call a program in another computer and pass it information. It also explains\nhow a called program can return a response.\nSOAP was developed by Microsoft, DevelopMentor, and Userland Software.\n\uf0b7 One of the advantages of SOAP is that program calls are more likely to get through\nfirewalls that normally screen out requests for those applications. Because HTTP\nrequests are normally allowed through firewalls, programs using SOAP can\ncommunicate with programs anywhere.\nK.NIKHILA Page 21\n\nUNIT III\nStandards are extremely important, and something that we take for granted\nthese days. For instance, it\u2019s nothing for us to email Microsoft Word documents\nback and forth and expect them to work on our computers.\nK.NIKHILA Page 22\n\nUNIT III\nSoftware as a Service\nSaaS (Software as a Service) is an application hosted on a remote server and accessed through\nthe Internet.\nAn easy way to think of SaaS is the web-based email service offered by such companies as\nMicrosoft (Hotmail), Google (Gmail), and Yahoo! (Yahoo Mail). Each mail service meets the\nbasic criteria: the vendor (Microsoft, Yahoo, and so on) hosts all of the programs and data in a\ncentral location, providing end users with access to the data and software, which is accessed\nacross the World Wide Web.\nSaaS can be divided into two major categories:\n\u2022 Line of business services these are business solutions offered to companies and enterprises.\nThey are sold via a subscription service. Applications covered under this category include\nbusiness processes, like supply- chain management applications, customer relations\napplications, and similar business-oriented tools.\n\u2022 Customer-oriented services These services are offered to the general public on a subscription\nbasis. More often than not, however, they are offered for free and supported by advertising.\nExamples in this category include the aforementioned web mail services, online gaming, and\nconsumer banking, among others.\nK.NIKHILA Page 23\n\nUNIT III\nAdvantages\n\u2022 There\u2019s a faster time to value and improved productivity, when\ncompared to the long implementation cycles and failure rate of\nenterprise software.\n\u2022 There are lower software licensing costs. \u2022 SaaS offerings feature the biggest\ncost savings over installed software by eliminating\nthe need for enterprises to install and maintain hardware, pay labor costs,\nand maintain the applications.\n\u2022 SaaS can be used to avoid the custom development cycles to get applications to\nthe organization quickly.\n\u2022 SaaS vendors typically have very meticulous security audits. SaaS vendors allow\ncompanies to have the most current version of an application as possible. This allows\nthe organization to spend their development dollars on new innovation in their\nindustry, rather than supporting old versions of applications.\nSaaS, on the other hand, has no licensing. Rather than buying the application, you pay for it\nthrough the use of a subscription, and you only pay for what you use. If you stop using the\napplication, you stop paying.\n3.3.2.Vendor Advantages\n\uf0b7 SaaS is advantage to Vendors also.And financial benefit is the top one\u2014vendors get a\nconstant stream of income, often what is more than the traditional software\nlicensing setup. Additionally, through SaaS, vendors can fend off piracy concerns and\nunlicensed use of software.\n\uf0b7 Vendors also benefit more as more subscribers come online. They have a huge\ninvestment in physical space, hardware, technology staff, and process development.\nK.NIKHILA Page 24\n\nUNIT III\nThe more these resources are used to capacity, the more the provider can clear as\nmargin.\nVirtualization Benefits\n\uf0b7 Virtualization makes it easy to move to a SaaS system. One of the main reasons is that it\nis easier for independent software vendors (ISVs) to adopt SaaS is the growth of\nvirtualization. The growing popularity of some SaaS vendors using Amazon\u2019s EC2 cloud\nplatform and the overall popularity of virtualized platforms help with the development of\nSaaS.\n3.3.3.Companies Offering SaaS Intuit\n\uf0b7 QuickBooks has been around for years as a conventional application for tracking\nbusiness accounting. With the addition of QuickBooks online, accounting has moved to\nthe cloud. QuickBooks Overview QuickBooks Online (www.qboe.com) gives small\nbusiness owners the ability to access their financial data whether they are at work,\nhome, or on the road. Intuit Inc. says the offering also gives users a high level of security\nbecause data is stored on firewall-protected servers and protected via automatic data\nbackups.\n\uf0b7 There is also no need to hassle with technology\u2014software upgrades are included at no\nextra charge.\n\uf0b7 For companies that are growing, QuickBooks Online Plus offers advanced features\nsuch as automatic billing and time tracking, as well as the ability to share information\nwith employees in multiple locations.\n\uf0b7 QuickBooks Online features include :\n\u2022 The ability to access financial data anytime and from anywhere. QuickBooks Online is\naccessible to users 24 hours a day, seven days a week.\n\u2022 Automated online banking. Download bank and credit card transactions automatically\nevery night, so it\u2019s easy to keep data up to date.\n\u2022 Reliable automatic data backup. Financial data is automatically backed up every day and is\nstored on Intuit\u2019s firewall-protected servers, which are monitored to keep critical business\ninformation safe and secure. QuickBooks Online also supports 128-bit Secure Sockets Layer\nK.NIKHILA Page 25\n\nUNIT III\n(SSL) encryption.\n\u2022 No software to buy, install, or maintain and no network required. The software is hosted\nonline, so small business users never have to worry about installing new software or upgrades.\nQuickBooks Online remembers customer, product, and vendor information, so users don\u2019t\nhave to re-enter data.\n\u2022 Easy accounts receivable and accounts payable. Invoice customers and track customer\npayments. Create an invoice with the click of a button. Apply specific credits to invoices or\napply a single-customer payment to multiple jobs or invoices. Receive bills and enter them\ninto QuickBooks Online with the expected due date.\n\u2022 Write and print checks. Enter information in the onscreen check form and print checks.\nGoogle\n\uf0b7 Google\u2019s SaaS offerings include Google Apps and Google Apps Premier Edition.\nGoogle Apps, launched as a free service in August 2006, is a suite of applications that includes\nGmail webmail services, Google Calendar shared calendaring, Google Talk instant messaging\nand Voice over IP, and the Start Page feature for creating a customizable home page on a\nspecific domain.\n\uf0b7 Google also offers Google Docs and Spreadsheets for all levels of Google Apps.\nAdditionally, Google Apps supports Gmail for mobile on BlackBerry handheld\ndevices.\nGoogle Apps Premier Edition has the following unique features:\n\u2022 Per-user storage of 10GBs Offers about 100 times the storage of the average corporate\nmailbox.\n\u2022 APIs for business integration APIs for data migration, user provisioning, single sign-on, and\nmail gateways enable businesses to further customize the service for unique environments.\n\u2022 Uptime of 99.9 percent Service level agreements for high availability of Gmail, with Google\nmonitoring and crediting customers if service levels are not met.\n\u2022 Advertising optional Advertising is turned off by default, but businesses can choose to\ninclude Google\u2019s relevant target-based ads if desired.\n\u2022 Low fee Simple annual fee of $50 per user account per year makes it practical to offer these\nK.NIKHILA Page 26\n\nUNIT III\napplications to select users in the organization.\nMicrosoft\nMicrosoft Office Live Small Business offers features including Store Manager, an e-commerce\ntool to help small businesses easily sell products on their own web site and on eBay; and E-\nmail Marketing beta, to make sending email newsletters and promotions simple and\naffordable.\nThe following features are available in Microsoft Office Live Small Business:\n\u2022 Store Manager is a hosted e-commerce service that enables users to easily sell products on\ntheir own web site and on eBay.\n\u2022 Custom domain name and business email is available to all customers for free for one year.\nPrivate domain name registration is included to help customers protect their contact\ninformation from spammers. Business email now includes 100 company-branded accounts,\neach with 5GB of storage.\n\u2022 Web design capabilities, including the ability to customize the entire page, as well as the\nheader, footer, navigation, page layouts, and more.\n\u2022 Support for Firefox 2.0 means Office Live Small Business tools and features are now compatible\nwith Macs.\n\u2022 A simplified sign-up process allows small business owners to get started quickly. Users do not\nhave to choose a domain name at sign-up or enter their credit card information.\n\u2022 Domain flexibility allows businesses to obtain their domain name through any provider\nand redirect it to Office Live Small Business. In addition, customers may purchase\nadditional domain names.\n\u2022 Synchronization with Microsoft Office Outlook provides customers with access to vital\nbusiness information such as their Office Live Small Business email, contacts, and calendars,\nboth online and offline.\n\u2022 E-mail Marketing beta enables users to stay connected to current customers and introduce\nthemselves to new ones by sending regular email newsletters, promotions, and updates.\nIBM\nK.NIKHILA Page 27\n\nUNIT III\nBig Blue\u2014IBM offers its own SaaS solution under the name \u201cBlue Cloud.\u201d\nBlue Cloud is a series of cloud computing offerings that will allow corporate datacenters to\noperate more like the Internet by enabling computing across a distributed, globally\naccessible fabric of resources, rather than on local machines or remote server farms.\nBlue Cloud is based on open-standards and open-source software supported by IBM\nsoftware, systems technology, and services. IBM\u2019s Blue Cloud development is supported by\nmore than 200 IBM Internet-scale researchers worldwide and targets clients who want to\nexplore the extreme scale of cloud computing infrastructures.\nSoftware plus Services\nSoftware plus Services takes the notion of Software as a Service (SaaS) to complement\npackaged software. Here are some of the ways in which it can help the client organization.\n3.4.1 Overview\n\uf0b7 User experience: Browsers have limitations as to just how rich the user experience can\nbe. Combining client software that provides the features we want with the ability of the\nInternet to deliver those experiences gives us the best of both worlds.\n\u2022 Working offline Not having to always work online gives us the flexibility to do our work,\nbut without the limitations of the system being unusable. By connecting occasionally and\nsynching data, we get a good solution for road warriors and telecommuters who don\u2019t have\nthe same bandwidth or can\u2019t always be connected.\n\u2022 Privacy worries: No matter how we use the cloud, privacy is a major concern. With\nSoftware plus Services, we can keep the most sensitive data housed on-site, while less\nK.NIKHILA Page 28\n\nUNIT III\nsensitive data can be moved to the cloud.\n\u2022 Marketing: Software plus Services gives vendors a chance to keep their names in front of\nclients. Since it\u2019s so easy to move from vendor to vendor, providing a part software/part-\nInternet solution makes it easier to sell our product to a client.\n\u2022 Power: More efficiency is realized by running software locally and synching to the cloud as\nneeded.\n\u2022 Flexibility: Vendors can offer software in different sizes and shapes\u2014whether onsite or\nhosted. This gives customers an opportunity to have the right-sized solution.\nSoftware plus Services offerings that prevalent companies have.\nVendors\ni. Microsoft : Microsoft offers Dynamics CRM, Microsoft Outlook, Windows Azure, and Azure\nServices Platform. Windows Azure is a collection of cloud-based services, including Live\nFramework, .NET Services, SQL Services, CRM Services, SharePoint Services, and Windows\nAzure Foundation Services for compute, storage, and management.\nii. Adobe: Adobe Integrated Runtime (AIR) brings Flash, ActionScript, and MXML/Flex to the\nPC. Using AIR, vendors can build desktop applications that access the cloud.\niii. Salesforce.com : Salesforce.com\u2019s AppExchange is a set of APIs that vendors can use to\ncreate desktop applications to access salesforce data and run on the desktop of an end\nuser.\niv. Apple: Apple offers a number of cloud-enabled features for its iPhone/iPod touch. Not only\ndoes it come with an integrated Safari web browser, but they also offer a software developer\u2019s\nkit (SDK) that allows software to be created for the iPhone/ iPod touch. Vendors can build their\nown applications, and on-the-go users can access cloud offerings with those applications.\nv. Google: Google\u2019s mobile platform is called \u201cAndroid\u201d and helps vendors build software for\nmobile phones. Google also offers its Google Apps and the Google Chrome browser, which also\ninstalls Google Gears software on the desktop. This allows offline and online solutions.\n3.4.2 Mobile Device Integration:\nHow Mobile Device Integration is done. How Microsoft Online provides this?\nA key component of Software plus Services is the ability to work in the cloud from a mobile\nK.NIKHILA Page 29\n\nUNIT III\ndevice. Google Android:\nA broad alliance of leading technology and wireless companies joined forces to develop Android,\nan open and comprehensive platform for mobile devices. Google Inc., T-Mobile, HTC, Qualcomm,\nMotorola, and others collaborated on the development of Android through the Open Handset\nAlliance, a multinational alliance of technology and mobile industry leaders.\nOpen Handset Alliance:\n\uf0b7 Thirty-four companies have formed the Open Handset Alliance, which aims to develop\ntechnologies that will significantly lower the cost of developing and distributing mobile\ndevices and services. The Android platform is the first step in this direction\u2014a fully\nintegrated mobile \u201csoftware stack\u201d that consists of an operating system, middleware,\nand user-friendly interface and applications. This alliance include major companied like,\n\u2022 Google (www.google.com)\n\u2022 HTC (www.htc.com)\n\u2022 Intel (www.intel.com)\n\u2022 LG (www.lge.com)\n\u2022 Marvell (www.marvell.com)\n\u2022 Motorola (www.motorola.com)\n\u2022 NMS Communications (www.nmscommunications.com)\n\u2022 NTT DoCoMo Inc. (www.nttdocomo.com)\n\u2022 Qualcomm (www.qualcomm.com)\n\u2022 Samsung (www.samsung.com) Etc...\n3.4.3. Providers:\nThe following development solutions we may consider for creating our own\nSoftware plus Services deployments.\na.Adobe AIR:\nAdobe Systems offers its Adobe Integrated Runtime (AIR), formerly code-named Apollo. Adobe\nAIR is a cross- operating-system application runtime that allows developers to use HTML/CSS,\nAJAX, Adobe Flash, and Adobe Flex to extend rich Internet applications (RIAs) to the desktop.\n\uf0b7 For its popular iPhone and iPod touch devices, Apple offers its iPhone Software\nK.NIKHILA Page 30\n\nUNIT III\nDevelopment Kit (SDK) as well as enterprise features such as support for Microsoft\nExchange ActiveSync to provide secure, over-the-air push email, contacts, and\ncalendars as well as remote wipe, and the addition of Cisco IPsec VPN for encrypted\naccess to private corporate networks.\nApp Store:\nThe iPhone software contains the App Store, an application that lets users browse, search,\npurchase, and wirelessly download third-party applications directly onto their iPhone or iPod\ntouch. The App Store enables developers to reach every iPhone and iPod touch user.\nDevelopers set the price for their applications (including free) and retain 70 percent of all sales\nrevenues. Users can download free applications at no charge to either the user or developer, or\npurchase priced applications with just one click. Enterprise customers can create a secure,\nprivate page on the App Store accessible only by their employees.\n3.4.4 Microsoft Online:\nMicrosoft provides Software plus Services offerings, integrating some of its most popular and\nprevalent offerings, like Exchange. Not only does Microsoft\u2019s Software plus Services offering\nallow a functional way to serve our organization, but it also provides a means to function on the\ncloud in simple way.\nHybrid Model\nWith Microsoft services like Exchange Online, SharePoint Online, and CRM 4.0,\norganizations big and small have more choices in how they access and manage enterprise from\nentirely web-based, to entirely on-premise solutions, and anywhere in between. Having a\nvariety of solutions to choose from gives customers the mobility and flexibility they need to\nmeet constantly evolving business needs. To meet this demand, Microsoft is moving toward a\nhybrid strategy of Software plus Services, the goal of which is to empower customers and\npartners with richer applications, more choices, and greater opportunity through a combination\nof on-premise software, partner-hosted software, and Microsoft-hosted software. As part of\nthis strategy, Microsoft expanded its Microsoft Online Services which includes Exchange Online\nand SharePoint Online to organizations of all sizes. With services like Microsoft Online Services\nK.NIKHILA Page 31\n\nUNIT III\nand Microsoft Dynamics CRM 4.0, organizations will have the flexibility required to address\ntheir business needs.\nExchange Online and SharePoint Online\nExchange Online and SharePoint Online are two examples of how partners can extend their\nreach, grow their revenues, and increase the number to sales in a Microsoft-hosted scenario. In\nSeptember 2007, Microsoft initially announced the worldwide availability of Microsoft Online\nServices\u2014which includes Exchange Online, SharePoint Online, Office Communications Online,\nand Office Live Meeting\u2014to organizations with more than 5,000 users. The extension of these\nservices to small and mid-sized businesses is appealing to partners in the managed services\nspace because they see it as an opportunity to deliver additional services and customer value\non top of Microsoft-hosted Exchange Online or SharePoint Online. Microsoft Online Services\nopens the door for partners to deliver reliable business services such as desktop and mobile\nemail, calendaring and contacts, instant messaging, audio and video conferencing, and shared\nworkspaces\u2014all of which will help increase their revenue stream and grow their businesses.\nMicrosoft Dynamics CRM 4.0:\nMicrosoft Dynamics CRM 4.0, released in December of 2007 which provides a key aspect of\nMicrosoft\u2019s Software plus Services strategy. The unique advantages of the new Microsoft\nDynamics CRM 4.0, which can be delivered on-premise or on-demand as a hosted solution,\nmake Microsoft Dynamics CRM an option for solution providers who want to rapidly offer a\nsolution that meets customer needs and maximizes their potential to grow their own business\nthrough additional services.\nK.NIKHILA Page 32\n\nUNIT IV\n4.1 Developing Applications\nIn cloud computing we can develop our own applications to\ncater the needs of our business. A simple example is\ndeveloping an app using Android to meet our business using\nGoogle App Engine and deploy in App Store. Similarly we\nmay use Intuit\u2019s QuickBase which allows us to develop\nfinancial-based cloud apps.\n4.1.1 Google\nTo develop an app on the cloud, the Google App Engine is the\nperfect tool to use, to make this dream become reality. In\nessence, we will write a bit of code in Python, tweak some\nHTML code, and then we have our app built, and it only takes\na few minutes. Using Google App Engine we can develop our\napplications without worry about buying servers, load\nbalancers, or DNS tables. Salesforce.com struck up a strategic\nalliance with Google with the availability of Force.com for\nGoogle App Engine. Force.com for Google App Engine is a set\nof tools and services to enable developer success with\napplication development in the cloud. The offering brings\ntogether Force.com and Google App Engine, enabling the\ncreation of entirely new web and business applications.\nForce.com for Google App Engine builds on the relationship\nbetween Salesforce.com and Google, spanning philanthropy,\nbusiness applications, social networks, and cloud computing.\na) Google Gears\nAnother development tool that Google offers is Google\nGears, an open-source technology for creating offline web\napplications. This browser extension was made available in its\nearly stages so that the development community could test its\ncapabilities and limitations and help Google improve upon it.\nGoogle\u2019s long-term hope is that Google Gears can help the\nindustry as a whole move toward a single standard for offline\ncapabilities that all developers can use.\n\nGears provide three key features:\n\uf0b7 A local server, to cache and serve application resources\n(HTML, JavaScript, images, etc.) without needing to contact a server\n\uf0b7 A database, to store and access data from within the browser\n\uf0b7 A worker thread pool, to make web applications more\nresponsive by performing expensive operations in the\nbackground .\n4.1.2 Microsoft\nMicrosoft\u2019s Azure Services Platform is a tool provided for\ndevelopers who want to write applications that are going to\nrun partially or entirely in a remote datacenter. The Azure\nServices Platform (Azure) is an Internet-scale cloud services\nplatform hosted in Microsoft datacenters, which provides an\noperating system and a set of developer services that can be\nused individually or together. Azure can be used to build new\napplications to run from the cloud or to enhance existing\napplications with cloud-based capabilities, and it forms the\nfoundation of all Microsoft\u2019s cloud offerings. Its open\narchitecture gives developers the choice to build web\napplications, applications running on connected devices, PCs,\nservers, or hybrid solutions offering the best of online and on\npremises.\nMicrosoft also offers cloud applications ready for consumption\nby customers such as Windows Live, Microsoft Dynamics,\nand other Microsoft Online Services for business such as\nMicrosoft Exchange Online and SharePoint Online. The\nAzure Services Platform lets developers provide their own\nunique customer offerings by offering the foundational\ncomponents of compute, storage, and building block services to\nauthor and compose applications in the cloud. Azure utilizes\nseveral other Microsoft services as part of its platform, known\nas the Live Mesh platform.\na) Live Services:\nLive Services is a set of building blocks within the Azure\nServices Platform that is used to handle user data and\napplication resources. Live Services provides developers with a\n\nway to build social applications and experiences across a range\nof digital devices that can connect with one of the largest\naudiences on the Web.\nb) Microsoft SQL Services:\nMicrosoft SQL Services enhances the capabilities of Microsoft\nSQL Server into the cloud as a web-based, distributed\nrelational database. It provides web services that enable\nrelational queries, search, and data synchronization with\nmobile users, remote offices, and business partners.\nc) Microsoft .NET Services :\n\uf0b7 Microsoft .NET Services is a tool for developing loosely\ncoupled cloud-based applications. .NET Services includes access\ncontrol to help secure applications, a service bus for\ncommunicating across applications and services, and hosted\nworkflow execution. These hosted services allow the creation of\napplications that span from on-premises environments to the\ncloud.\n\uf0b7 Microsoft SharePoint Services and Dynamics CRM\nServices are used to allow developers to collaborate and build\nstrong customer relationships. Using tools like Visual Studio,\ndevelopers can build applications that utilize SharePoint and\nCRM capabilities.\nd) Microsoft Azure Design\n\uf0b7 Azure is designed in several layers, with different things going\non under the hood, Layer Zero\n\uf0b7 Layer Zero is Microsoft\u2019s Global Foundational Service.\nGFS is akin to the hardware abstraction layer (HAL) in\nWindows. It is the most basic level of the software that\ninterfaces directly with the servers.\nLayer One\nLayer One is the base Azure operating system. It used to be code-\nnamed \u201cRed Dog,\u201d and was designed by a team of operating system\nexperts at Microsoft. Red Dog is the technology that networks and\nmanages the Windows Server 2008 machines that form the\n\nMicrosoft hosted cloud.\nRed Dog is made up of four pillars:\n\u2022 Storage (a file system)\n\u2022 The fabric controller, which is a management system for\ndeploying and provisioning\n\u2022 Virtualized computation/VM\n\u2022 Development environment, which allows developers to\nemulate Red dog on their desktops\nLayer Two:\nLayer Two provides the building blocks that run on Azure.\nThese services are the aforementioned Live Mesh platform.\nDevelopers build on top of these lower-level services when building\ncloud apps.\nSharePoint Services and CRM Services are not the same as\nSharePoint Online and CRM Online. They are just the platform\nbasics that do not include user interface elements.\nLayer Three\nAt Layer Three exist the Azure-hosted applications. Some of the\napplications developed by Microsoft include SharePoint Online,\nExchange Online, Dynamics CRM, and Online. Third parties will\ncreate other applications.\n4.1.3 Intuit QuickBase:\nIntuit Inc.\u2019s QuickBase launched its new QuickBase\nBusiness Consultant Program. The program allows members to use\ntheir expertise to create unique business applications tailored\nspecifically to the industries they serve\u2014without technical expertise\nor coding. This helps members expand their reach into industries\nformerly served only by IT experts. Using QuickBase, program\nmembers will be able to easily build new on- demand business\napplications from scratch or customize one of 200 available templates\nand resell them to their clients.\n\n4.2 HADOOP\nWhat??\nUsing the solution provided by Google, Doug Cutting and his team\ndeveloped an Open Source Project called HADOOP\nWhy??\nHadoop runs applications using the MapReduce algorithm, where the\ndata is processed in parallel with others. In short, Hadoop is used to\ndevelop applications that could perform complete statistical analysis\non huge amounts of data.\nHadoop is an Apache open source framework written in java\nthat allows distributed processing of large datasets across clusters of\ncomputers using simple programming models. The Hadoop\nframework application works in an environment that provides\ndistributed storage and computation across clusters of computers.\nHadoop is designed to scale up from single server to thousands of\nmachines, each offering local computation and storage.\nHadoop Ecosystem\nIntroduction: Hadoop Ecosystem is a platform or a suite which\nprovides various services to solve the big data problems. It includes\nApache projects and various commercial tools and solutions. There\n\nare four major elements of Hadoop i.e. HDFS, MapReduce, YARN, and\nHadoop Common. Most of the tools or solutions are used to\nsupplement or support these major elements. All these tools work\ncollectively to provide services such as absorption, analysis, storage\nand maintenance of data etc.\nFollowing are the components that collectively form a Hadoop\necosystem:\n\uf0b7 HDFS: Hadoop Distributed File System\n\uf0b7 YARN: Yet Another Resource Negotiator\n\uf0b7 MapReduce: Programming based Data Processing\n\uf0b7 Spark: In-Memory data processing\n\uf0b7 PIG, HIVE: Query based processing of data services\n\uf0b7 HBase: NoSQL Database\n\uf0b7 Mahout, Spark MLLib: Machine Learning algorithm libraries\n\uf0b7 Solar, Lucene: Searching and Indexing\n\uf0b7 Zookeeper: Managing cluster\n\uf0b7 Oozie: Job Scheduling\n\nAll these toolkits or components revolve around one term i.e. Data.\nThat\u2019s the beauty of Hadoop that it revolves around data and hence\nmaking its synthesis easier.\nHDFS:\n\uf0b7 HDFS is the primary or major component of Hadoop\necosystem and is responsible for storing large data sets of structured\nor unstructured data across various nodes and thereby maintaining\nthe metadata in the form of log files.\n\uf0b7 HDFS consists of two core components i.e.\n1. Name node\n2. Data Node\n\uf0b7 Name Node is the prime node which contains metadata (data\nabout data) requiring comparatively fewer resources than the data\nnodes that stores the actual data. These data nodes are commodity\nhardware in the distributed environment. Undoubtedly, making\nHadoop cost effective.\n\uf0b7 HDFS maintains all the coordination between the clusters and\nhardware, thus working at the heart of the system.\n\nYARN:\n\uf0b7 Yet Another Resource Negotiator, as the name implies, YARN\nis the one who helps to manage the resources across the clusters. In\nshort, it performs scheduling and resource allocation for the Hadoop\nSystem.\n\uf0b7 Consists of three major components i.e.\n1. Resource Manager\n2. Nodes Manager\n3. Application Manager\n\uf0b7 Resource manager has the privilege of allocating resources for\nthe applications in a system whereas Node managers work on the\nallocation of resources such as CPU, memory, bandwidth per machine\nand later on acknowledges the resource manager. Application\nmanager works as an interface between the resource manager and\nnode manager and performs negotiations as per the requirement of the\ntwo.\nMapReduce:\n\uf0b7 By making the use of distributed and parallel algorithms,\nMapReduce makes it possible to carry over the processing\u2019s logic and\nhelps to write applications which transform big data sets into a\nmanageable one.\n\uf0b7 MapReduce makes the use of two functions i.e. Map() and\nReduce() whose task is:\n1. Map() performs sorting and filtering of data and\nthereby organizing them in the form of group. Map generates a key-\nvalue pair based result which is later on processed by the Reduce()\nmethod.\n2. Reduce(), as the name suggests does the summarization\nby aggregating the mapped data. In simple, Reduce() takes the output\ngenerated by Map() as input and combines those tuples into smaller\nset of tuples.\nPig:\n\uf0b7 Pig was basically developed by Yahoo which works on a pig\nLatin language, which is Query based language similar to SQL.\n\n\uf0b7 It is a platform for structuring the data flow, processing and\nanalyzing huge data sets.\n\uf0b7 Pig does the work of executing commands and in the\nbackground, all the activities of MapReduce are taken care of. After\nthe processing, pig stores the result in HDFS.\n\uf0b7 Pig Latin language is specially designed for this framework\nwhich runs on Pig Runtime. Just the way Java runs on the JVM.\n\uf0b7 Pig helps to achieve ease of programming and optimization and\nhence is a major segment of the Hadoop Ecosystem.\nHive:\n\uf0b7 With the help of SQL methodology and interface, HIVE\nperforms reading and writing of large data sets. However, its query\nlanguage is called as HQL (Hive Query Language).\n\uf0b7 It is highly scalable as it allows real-time processing and batch\nprocessing both. Also, all the SQL datatypes are supported by Hive\nthus, making the query processing easier.\n\uf0b7 Similar to the Query Processing frameworks, HIVE too comes\nwith two components: JDBC Drivers and HIVE Command Line.\n\uf0b7 JDBC, along with ODBC drivers work on establishing the data\nstorage permissions and connection whereas HIVE Command line\nhelps in the processing of queries.\nMahout:\n\uf0b7 Mahout, allows Machine Learnability to a system or\napplication. Machine Learning, as the name suggests helps the system\nto develop itself based on some patterns, user/environmental\ninteraction or on the basis of algorithms.\n\uf0b7 It provides various libraries or functionalities such as\ncollaborative filtering, clustering, and classification which are nothing\nbut concepts of Machine learning. It allows invoking algorithms as\nper our need with the help of its own libraries.\nApache Spark:\n\uf0b7 It\u2019s a platform that handles all the process consumptive tasks\nlike batch processing, interactive or iterative real-time processing,\ngraph conversions, and visualization, etc.\n\n\uf0b7 It consumes in memory resources hence, thus being faster than\nthe prior in terms of optimization.\n\uf0b7 Spark is best suited for real-time data whereas Hadoop is best\nsuited for structured data or batch processing, hence both are used in\nmost of the companies interchangeably.\nApache HBase:\n\uf0b7 It\u2019s a NoSQL database which supports all kinds of data and\nthus capable of handling anything of Hadoop Database. It provides\ncapabilities of Google\u2019s BigTable, thus able to work on Big Data sets\neffectively.\n\uf0b7 At times where we need to search or retrieve the occurrences of\nsomething small in a huge database, the request must be processed\nwithin a short quick span of time. At such times, HBase comes handy\nas it gives us a tolerant way of storing limited data.\nOther Components: Apart from all of these, there are some other\ncomponents too that carry out a huge task in order to make Hadoop\ncapable of processing large datasets. They are as follows:\n\uf0b7 Solr, Lucene: These are the two services that perform the task\nof searching and indexing with the help of some java libraries,\nespecially Lucene is based on Java which allows spell check\nmechanism, as well. However, Lucene is driven by Solr.\n\uf0b7 Zookeeper: There was a huge issue of management of\ncoordination and synchronization among the resources or the\ncomponents of Hadoop which resulted in inconsistency, often.\nZookeeper overcame all the problems by performing synchronization,\ninter-component based communication, grouping, and maintenance.\n\uf0b7 Oozie: Oozie simply performs the task of a scheduler, thus\nscheduling jobs and binding them together as a single unit. There is\ntwo kinds of jobs .i.e Oozie workflow and Oozie coordinator jobs. Oozie\nworkflow is the jobs that need to be executed in a sequentially ordered\nmanner whereas Oozie Coordinator jobs are those that are triggered\nwhen some data or external stimulus is given to it.\nHadoop Architecture:\nAt its core, Hadoop has two major layers namely \u2212\n\n\uf0b7 Processing/Computation layer (MapReduce), and\n\uf0b7 Storage layer (Hadoop Distributed File System).\nHow Does Hadoop Work?\nIt is quite expensive to build bigger servers with heavy\nconfigurations that handle large scale processing, but as an alternative,\nyou can tie together many commodity computers with single-CPU, as\na single functional distributed system and practically, the clustered\nmachines can read the dataset in parallel and provide a much higher\nthroughput. Moreover, it is cheaper than one high-end server. So this\nis the first motivational factor behind using Hadoop that it runs\nacross clustered and low-cost machines.\nHadoop runs code across a cluster of computers. This process includes\nthe following core tasks that Hadoop performs \u2212\n\uf0b7 Data is initially divided into directories and files. Files are\ndivided into uniform sized blocks of 128M and 64M (preferably\n128M).\n\uf0b7 These files are then distributed across various cluster nodes for\nfurther processing.\n\n\uf0b7 HDFS, being on top of the local file system, supervises the\nprocessing.\n\uf0b7 Blocks are replicated for handling hardware failure.\n\uf0b7 Checking that the code was executed successfully.\n\uf0b7 Performing the sort that takes place between the map and\nreduce stages.\n\uf0b7 Sending the sorted data to a certain computer.\n\uf0b7 Writing the debugging logs for each job.\nAdvantages of Hadoop\n\uf0b7 Hadoop framework allows the user to quickly write and test\ndistributed systems. It is efficient, and it automatic distributes the\ndata and work across the machines and in turn, utilizes the underlying\nparallelism of the CPU cores.\n\uf0b7 Hadoop does not rely on hardware to provide fault-tolerance\nand high availability (FTHA), rather Hadoop library itself has been\ndesigned to detect and handle failures at the application layer.\n\uf0b7 Servers can be added or removed from the cluster dynamically\nand Hadoop continues to operate without interruption.\n\uf0b7 Another big advantage of Hadoop is that apart from being open\nsource, it is compatible on all the platforms since it is Java based.\n\uf0b7 Hadoop is supported by GNU/Linux platform and its flavors.\nTherefore, we have to install a Linux operating system for setting up\nHadoop environment. In case you have an OS other than Linux, you\ncan install a Virtualbox software in it and have Linux inside the\nVirtualbox.\nMapReduce\n\n\uf0b7 MapReduce is a parallel programming model for writing\ndistributed applications devised at Google for efficient processing of\nlarge amounts of data (multi-terabyte data-sets), on large clusters\n(thousands of nodes) of commodity hardware in a reliable, fault-\ntolerant manner. The MapReduce program runs on Hadoop which is\nan Apache open-source framework.\n\uf0b7 MapReduce is a framework using which we can write\napplications to process huge amounts of data, in parallel, on large\nclusters of commodity hardware in a reliable manner.\nWhat is MapReduce?\n\uf0b7 MapReduce is a processing technique and a program model for\ndistributed computing based on java. The MapReduce algorithm\ncontains two important tasks, namely Map and Reduce. Map takes a\nset of data and converts it into another set of data, where individual\nelements are broken down into tuples (key/value pairs). Secondly,\nreduce task, which takes the output from a map as an input and\ncombines those data tuples into a smaller set of tuples. As the\nsequence of the name MapReduce implies, the reduce task is always\nperformed after the map job.\n\uf0b7 The major advantage of MapReduce is that it is easy to scale\ndata processing over multiple computing nodes. Under the\nMapReduce model, the data processing primitives are called mappers\nand reducers. Decomposing a data processing application\ninto mappers and reducers is sometimes nontrivial. But, once we write\nan application in the MapReduce form, scaling the application to run\nover hundreds, thousands, or even tens of thousands of machines in a\n\ncluster is merely a configuration change. This simple scalability is\nwhat has attracted many programmers to use the MapReduce model.\nThe Algorithm\n\uf0b7 Generally MapReduce paradigm is based on sending the\ncomputer to where the data resides!\n\uf0b7 MapReduce program executes in three stages, namely map\nstage, shuffle stage, and reduce stage.\no Map stage \u2212 The map or mapper\u2019s job is to process the\ninput data. Generally the input data is in the form of file or directory\nand is stored in the Hadoop file system (HDFS). The input file is\npassed to the mapper function line by line. The mapper processes the\ndata and creates several small chunks of data.\no Reduce stage \u2212 This stage is the combination of\nthe Shuffle stage and the Reduce stage. The Reducer\u2019s job is to\nprocess the data that comes from the mapper. After processing, it\nproduces a new set of output, which will be stored in the HDFS.\n\uf0b7 During a MapReduce job, Hadoop sends the Map and Reduce\ntasks to the appropriate servers in the cluster.\n\uf0b7 The framework manages all the details of data-passing such as\nissuing tasks, verifying task completion, and copying data around the\ncluster between the nodes.\n\uf0b7 Most of the computing takes place on nodes with data on local\ndisks that reduces the network traffic.\n\uf0b7 After completion of the given tasks, the cluster collects and\nreduces the data to form an appropriate result, and sends it back to\nthe Hadoop server.\n\nInputs and Outputs (Java Perspective)\n\uf0b7 The MapReduce framework operates on <key, value> pairs,\nthat is, the framework views the input to the job as a set of <key,\nvalue> pairs and produces a set of <key, value> pairs as the output of\nthe job, conceivably of different types.\n\uf0b7 The key and the value classes should be in serialized manner\nby the framework and hence, need to implement the Writable\ninterface. Additionally, the key classes have to implement the\nWritable-Comparable interface to facilitate sorting by the\nframework. Input and Output types of a MapReduce job \u2212 (Input)\n<k1, v1> \u2192 map \u2192 <k2, v2> \u2192 reduce \u2192 <k3, v3>(Output).\nInput Output\nMap <k1, v1> list (<k2, v2>)\nReduce <k2, list(v2)> list (<k3, v3>)\n\nTerminology\n\uf0b7 PayLoad \u2212 Applications implement the Map and the Reduce\nfunctions, and form the core of the job.\n\uf0b7 Mapper \u2212 Mapper maps the input key/value pairs to a set of\nintermediate key/value pair.\n\uf0b7 NameNode \u2212 Node that manages the Hadoop Distributed File\nSystem (HDFS).\n\uf0b7 DataNode \u2212 Node where data is presented in advance before\nany processing takes place.\n\uf0b7 Master Node \u2212 Node where JobTracker runs and which\naccepts job requests from clients.\n\uf0b7 Slave Node \u2212 Node where Map and Reduce program runs.\n\uf0b7 Job Tracker \u2212 Schedules jobs and tracks the assign jobs to Task\ntracker.\n\uf0b7 Task Tracker \u2212 Tracks the task and reports status to\nJobTracker.\n\uf0b7 Job \u2212 A program of an execution of a Mapper and Reducer\nacross a dataset.\n\uf0b7 Task \u2212 an execution of a Mapper or a Reducer on a slice of\ndata.\n\uf0b7 Task Attempt \u2212 A particular instance of an attempt to execute\na task on a Slave Node.\nMapReduce Tutorial: A Word Count Example of MapReduce\nLet us understand how a MapReduce works by taking an example\nwhere I have a text file called example.txt whose contents are as\nfollows:\n\nDear, Bear, River, Car, Car, River, Deer, Car and Bear\nNow, suppose, we have to perform a word counts on the sample.txt\nusing MapReduce. So, we will be finding the unique words and the\nnumber of occurrences of those unique words.\n\uf0b7 First, we divide the input in three splits as shown in the figure.\nThis will distribute the work among all the map nodes.\n\uf0b7 Then, we tokenize the words in each of the mapper and give a\nhardcoded value (1) to each of the tokens or words. The rationale\nbehind giving a hardcoded value equal to 1 is that every word, in itself,\nwill occur once.\n\uf0b7 Now, a list of key-value pair will be created where the key is\nnothing but the individual words and value is one. So, for the first\nline (Dear Bear River) we have 3 key-value pairs \u2013 Dear, 1; Bear, 1;\nRiver, 1. The mapping process remains the same on all the nodes.\n\uf0b7 After mapper phase, a partition process takes place where\nsorting and shuffling happens so that all the tuples with the same key\nare sent to the corresponding reducer.\n\uf0b7 So, after the sorting and shuffling phase, each reducer will have\na unique key and a list of values corresponding to that very key. For\nexample, Bear, [1,1]; Car, [1,1,1].., etc.\n\uf0b7 Now, each Reducer counts the values which are present in that\nlist of values. As shown in the figure, reducer gets a list of values\n\nwhich is [1,1] for the key Bear. Then, it counts the number of ones in\nthe very list and gives the final output as \u2013 Bear, 2.\n\uf0b7 Finally, all the output key/value pairs are then collected and\nwritten in the output file.\nHadoop Distributed File System\n\uf0b7 The Hadoop Distributed File System (HDFS) is based on the\nGoogle File System (GFS) and provides a distributed file system that\nis designed to run on commodity hardware. It has many similarities\nwith existing distributed file systems. However, the differences from\nother distributed file systems are significant. It is highly fault-tolerant\nand is designed to be deployed on low-cost hardware. It provides high\nthroughput access to application data and is suitable for applications\nhaving large datasets.\nApart from the above-mentioned two core components, Hadoop\nframework also includes the following two modules \u2212\n\uf0b7 Hadoop Common \u2212 these are Java libraries and utilities\nrequired by other Hadoop modules.\n\uf0b7 Hadoop YARN \u2212 this is a framework for job scheduling and\ncluster resource management.\nHadoop File System was developed using distributed file system\ndesign. It is run on commodity hardware. Unlike other distributed\nsystems, HDFS is highly fault tolerant and designed using low-cost\nhardware.\n\uf0b7 HDFS holds very large amount of data and provides easier\naccess. To store such huge data, the files are stored across multiple\nmachines. These files are stored in redundant fashion to rescue the\nsystem from possible data losses in case of failure. HDFS also makes\napplications available to parallel processing.\n\nFeatures of HDFS\ni) Fault Tolerance\nThe fault tolerance in Hadoop HDFS is the working strength of a\nsystem in unfavorable conditions. It is highly fault-tolerant. Hadoop\nframework divides data into blocks. After that creates multiple copies\nof blocks on different machines in the cluster. So, when any machine\nin the cluster goes down, then a client can easily access their data\nfrom the other machine which contains the same copy of data blocks.\nii) High Availability\nHadoop HDFS is a highly available file system. In HDFS, data gets\nreplicated among the nodes in the Hadoop cluster by creating a replica\nof the blocks on the other slaves present in HDFS cluster. So,\nwhenever a user wants to access this data, they can access their data\nfrom the slaves which contain its blocks. At the time of unfavorable\nsituations like a failure of a node, a user can easily access their data\nfrom the other nodes. Because duplicate copies of blocks are present\non the other nodes in the HDFS cluster.\niii) High Reliability\nHDFS provides reliable data storage. It can store data in the range of\n100s of petabytes. HDFS stores data reliably on a cluster. It divides\nthe data into blocks. Hadoop framework stores these blocks on nodes\npresent in HDFS cluster. HDFS stores data reliably by creating a\nreplica of each and every block present in the cluster. Hence provides\nfault tolerance facility. If the node in the cluster containing data goes\ndown, then a user can easily access that data from the other nodes.\nHDFS by default creates 3 replicas of each block containing data\npresent in the nodes. So, data is quickly available to the users. Hence\nuser does not face the problem of data loss. Thus, HDFS is highly\nreliable.\n\niv) Replication\nData Replication is unique features of HDFS. Replication solves the\nproblem of data loss in an unfavorable condition like hardware failure,\ncrashing of nodes etc. HDFS maintain the process of replication at\nregular interval of time. HDFS also keeps creating replicas of user\ndata on different machine present in the cluster. So, when any node\ngoes down, the user can access the data from other machines. Thus,\nthere is no possibility of losing of user data.\nv) Scalability\nHadoop HDFS stores data on multiple nodes in the cluster. So,\nwhenever requirements increase you can scale the cluster. Two\nscalability mechanisms are available in HDFS: Vertical and\nHorizontal Scalability.\nHDFS Architecture\nGiven below is the architecture of a Hadoop File System.\n\uf0b7 HDFS follows the master-slave architecture and it has the\nfollowing elements.\na) Namenode\nThe namenode is the commodity hardware that contains the\nGNU/Linux operating system and the namenode software. It is\nsoftware that can be run on commodity hardware. The system having\n\nthe namenode acts as the master server and it does the following tasks\n\u2212\n\uf0b7 Manages the file system namespace.\n\uf0b7 Regulates client\u2019s access to files.\n\uf0b7 It also executes file system operations such as renaming,\nclosing, and opening files and directories.\nb) Datanode\nThe datanode is a commodity hardware having the GNU/Linux\noperating system and datanode software. For every node (Commodity\nhardware/System) in a cluster, there will be a datanode. These nodes\nmanage the data storage of their system.\n\uf0b7 Datanodes perform read-write operations on the file systems,\nas per client request.\n\uf0b7 They also perform operations such as block creation, deletion,\nand replication according to the instructions of the namenode.\nc) Block\nGenerally the user data is stored in the files of HDFS. The file in a\nfile system will be divided into one or more segments and/or stored\nin individual data nodes. These file segments are called as blocks. In\nother words, the minimum amount of data that HDFS can read or\nwrite is called a Block. The default block size is 64MB, but it can be\nincreased as per the need to change in HDFS configuration.\nGoals of HDFS\ni) Fault detection and recovery \u2212 since HDFS includes a large number\nof commodity hardware, failure of components is frequent. Therefore\nHDFS should have mechanisms for quick and automatic fault\ndetection and recovery.\n\nii) Huge datasets \u2212 HDFS should have hundreds of nodes per cluster\nto manage the applications having huge datasets.\niii) Hardware at data \u2212 A requested task can be done efficiently, when\nthe computation takes place near the data. Especially where huge\ndatasets are involved, it reduces the network traffic and increases the\nthroughput.\n\nUNIT V\nHADOOP I/O\n\uf0b7 Hadoop comes with a set of primitives for data I/O and the techniques that are more general than Hadoop,\nsuch as data integrity and compression, but deserve special consideration when dealing with multi-terabyte\ndatasets.\n\uf0b7 Others are Hadoop tools or APIs that form the building blocks for developing distributed system, such as\nserialization frameworks and on-disk data structures\n5.1. Data integrity\nThe usual way of detecting corrupted data is by computing a checksum for the data when it first enters the system,\nand again whenever it is transmitted across a channel that is unreliable and hence capable of corrupting the data. The\ndata is deemed to be corrupt if the newly generated checksum doesn\u2019t exactly match the original. This technique\ndoesn\u2019t offer any way to fix the data\u2014merely error detection. A commonly used error-detecting code is CRC-32\n(cyclic redundancy check), which computes a 32-bit integer checksum for input of any size.\n5.1.1 Data Integrity in HDFS\n\uf0b7 HDFS transparently checksums all data written to it and by default verifies checksums when reading data. A\nseparate checksum is created for every io.bytes.per.checksum bytes of data. The default is 512 bytes, and since\na CRC-32 checksum is 4 bytes long, the storage overhead is less than 1%.\n\uf0b7 Datanodes are responsible for verifying the data they receive before storing the data and its checksum. This\napplies to data that they receive from clients and from other datanodes during replication. A client writing\ndata sends it to a pipeline of datanodes and the last datanode in the pipeline verifies the checksum. If it\ndetects an error, the client receives a ChecksumException, a subclass of IOException, which it should handle\nin an application-specific manner, by retrying the operation, for example.\n\uf0b7 When clients read data from datanodes, they verify checksums as well, comparing them with the ones stored\nat the datanode. Each datanode keeps a persistent log of checksum verifications, so it knows the last time each\nof its blocks was verified. When a client successfully verifies a block, it tells the datanode, which updates its\nlog. Keeping statistics such as these is valuable in detecting bad disks.\n\uf0b7 Aside from block verification on client reads, each datanode runs a DataBlockScanner in a background thread\nthat periodically verifies all the blocks stored on the datanode. This is to guard against corruption due to \u201cbit\nrot\u201d in the physical storage media. See \u201cDatanode block scanner\u201d for details on how to access the scanner\nreports.\n\uf0b7 Since HDFS stores replicas of blocks, it can \u201cheal\u201d corrupted blocks by copying one of the good replicas to\nproduce a new, uncorrupt replica. The way this works is that if a client detects an error when reading a block,\nit reports the bad block and the datanode it was trying to read from to the namenode before throwing a\nChecksumException. The namenode marks the block replica as corrupt, so it doesn\u2019t direct clients to it, or try\nto copy this replica to another datanode. It then schedules a copy of the block to be replicated on another\ndatanode, so its replication factor is back at the expected level. Once this has happened, the corrupt replica is\ndeleted.\n\n\uf0b7 It is possible to disable verification of checksums by passing false to the setVerify Checksum() method on\nFileSystem, before using the open() method to read a file. The same effect is possible from the shell by using\nthe -ignoreCrc option with the -get or the equivalent -copyToLocal command. This feature is useful if you\nhave a corrupt file that you want to inspect so you can decide what to do with it. For example, you might\nwant to see whether it can be salvaged before you delete it.\n5.1.2 LocalFileSystem\nThe Hadoop LocalFileSystem performs client-side checksumming. This means that when you write a file\ncalled filename, the filesystem client transparently creates a hidden file, .filename.crc, in the same directory\ncontaining the checksums for each chunk of the file. Like HDFS, the chunk size is controlled by the\nio.bytes.per.checksum property, which defaults to 512 bytes. The chunk size is stored as metadata in the .crc file, so\nthe file can be read back correctly even if the setting for the chunk size has changed. Checksums are verified when\nthe file is read, and if an error is detected, LocalFileSystem throws a ChecksumException.\nChecksums are fairly cheap to compute (in Java, they are implemented in native code), typically adding a few\npercent overhead to the time to read or write a file. For most pay for data integrity. It is, however, possible to disable\nchecksums: typically when the underlying filesystem supports checksums natively. This is accomplished by using\nRawLocalFileSystem in place of Local FileSystem. To do this globally in an application, it suffices to remap the\nimplementation for file URIs by setting the property fs.file.impl to the value\norg.apache.hadoop.fs.RawLocalFileSystem. Alternatively, you can directly create a Raw LocalFileSystem instance,\nwhich may be useful if you want to disable checksum verification for only some reads;\nFor example:\nConfiguration conf= ...\nFileSystemfs = new RawLocalFileSystem();\nfs.initialize(null, conf);\n5.1.3 ChecksumFileSystem\nLocalFileSystem uses ChecksumFileSystem to do its work, and this class makes it easy to add checksumming\nto other (nonchecksummed) filesystems, as Checksum FileSystem is just a wrapper around FileSystem. The general\nidiom is as follows:\nFileSystemrawFs= ...\nFileSystemchecksummedFs = new ChecksumFileSystem(rawFs);\nThe underlying filesystem is called the raw filesystem, and may be retrieved using the getRawFileSystem()\nmethod on checksumFileSystem. ChecksumFileSystem has a few more useful methods for working with checksums,\nsuch as getChecksumFile() for getting the path of a checksum file for any file. Check the documentation for the\nothers.\nIf an error is detected by ChecksumFileSystem when reading a file, it will call its reportChecksumFailure ()\nmethod. The default implementation does nothing, but LocalFileSystem moves the offending file and its checksum\nto a side directory on the same device called bad_files. Administrators should periodically check for these bad files\nand take action on them.\n\n5.2 Compression\n\uf0b7 All of the tools listed in Table 4-1 give some control over this trade-off at compression time by offering nine different\noptions\n-1 means optimize for speed and\n-9 means optimize for space\ne.g :--- gzip-1 file\nThe different tools have very different compression characteristics.\uf0d8Both gzip and ZIPare general-purpose\ncompressors, and sit in the middle of the space/time trade-off.\n\uf0d8 Bzip2compresses more effectively than gzipor ZIP, but is slower.\n\uf0d8 LZOoptimizes for speed. It is faster than gzipand ZIP, but compresses slightly less effectively\n5.2.1 Codecs\n\uf0b7 A codec is the implementation of a compression-decompression algorithm\n\uf0b7 The LZO libraries are GPL-licensed and may not be included in Apache distributions, so for this reason the\nHadoopcodecs must be downloaded separately from http://code.google.com/p/hadoop-gpl-compression/\nCompressing and decompressing streams with CompressionCodec\n\uf0b7 CompressionCodechas two methods that allow you to easily compress or decompress data.\n\uf0b7 To compress data being written to an output stream, use the createOutputStream(OutputStreamout) method to\ncreate a CompressionOutputStream to which you write your uncompressed data to have it written in\ncompressed form to the underlying stream.\n\uf0b7 To decompress data begin read from an input stream, call createIntputStream(InputStreamin) to obtain a\nCompressionInputStream, which allows you to read uncompressed data from the underlying stream.\nString codecClassname = args[0];\nClass<?>codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\n\nCompressionOutputStream out = codec.createOutputStream(System.out);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\nInferring CompressionCodecsusing CompressionCodecFactory\n\uf0b7 If you are reading a compressed file, you can normally infer the codec to use by looking at its filename extension. A\nfile ending in .gzcan be read with GzipCodec, and so on.\n\uf0b7 CompressionCodecFactoryprovides a way of mapping a filename extension to a compressionCodecusing its\ngetCodec() method, which takes a Path object for the file in question.\n\uf0b7 Following example shows an application that uses this feature to decompress files.\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystemfs = FileSystem.get(URI.create(uri), conf);\nPath inputPath = new Path(uri);\nCompressionCodecFactory factory = new CompressionCodecFactory(conf);\nCompressionCodec codec = factory.getCodec(inputPath);\nif (codec == null)\n{\nSystem.err.println(\"No codec found for \" + uri);\nSystem.exit(1);\n}\nString outputUri =\nCompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension());\nInputStream in = null;\nOutputStream out = null;\ntry {\nin = codec.createInputStream(fs.open(inputPath));\nout = fs.create(new Path(outputUri));\nIOUtils.copyBytes(in, out, conf);\n}\nFinally\n{\nIOUtils.closeStream(in);\nIOUtils.closeStream(out);\n}\nNative libraries\n\uf0b7 For performance, it is preferable to use a native library for compression and decompression. For example, in\none test, using the native gziplibraries reduced decompression times by up to 50%and compression times by\naround 10%(compared to the built-in Java implementation).\n\uf0b7 Hadoopcomes with prebuilt native compression libraries for 32-and 64-bit Linux, which you can find in the\nlib/native directory.\n\uf0b7 By default Hadooplooks for native libraries for the platform it is running on, and loads them automatically if\nthey are found.\n\nNative libraries \u2013CodecPool\n\uf0b7 If you are using a native library and you are doing a lot of compression or decompression in your application,\nconsider using CodecPool, which allows you to reuse compressors and decompressors, thereby amortizing the\ncost of creating these objects.\nString codecClassname = args[0];\nClass<?>codecClass = Class.forName(codecClassname);\nConfiguration conf = new Configuration();\nCompressionCodec codec = (CompressionCodec)\nReflectionUtils.newInstance(codecClass, conf);\nCompressor compressor = null;\ntry\n{\ncompressor = CodecPool.getCompressor(codec);\nCompressionOutputStream out =\ncodec.createOutputStream(System.out, compressor);\nIOUtils.copyBytes(System.in, out, 4096, false);\nout.finish();\n}\nfinally\n{\nCodecPool.returnCompressor(compressor);\n}\n5.2.2 Compression and Input Splits\n\uf0b7 When considering how to compress data that will be processed by MapReduce, it is important to understand\nwhether the compression format supports splitting.\n\uf0b7 Consider an uncompressed file stored in HDFS whose size is 1GB. With a HDFS block size of 64MB, the file\nwill be stored as 16 blocks, and a Map Reduce job using this file as input will create 16 input splits, each\nprocessed independently as input to a separate map task.\n\uf0b7 Imagine now the file is a gzip-compressed file whose compressed size is 1GB. As before, HDFS will store the\nfile as 16 blocks. However, creating a split for each block won\u2019t work since it is impossible to start reading at\nan arbitrary point in the gzipstream, and therefore impossible for a map task to read its split independently of\nthe others.\n\uf0b7 In this case, Map Reduce will do the right thing, and not try to split the gzippedfile.This will work, but at the\nexpense of locality. A single map will process the 16 HDFS blocks, most of which will not be local to the\nmap. Also, with fewer maps, the job is less granular, and so may take longer to run.\n\n5.2.3 Using Compression in MapReduce\nIf your input files are compressed, they will be automatically decompressed as they are read by MapReduce,\nusing the filename extension to determine the codec to use.\nFor Example...\nCompressing map output\n\uf0b7 Even if your Map Reduce application reads and writes uncompressed data, it may\nbenefit from compressing the intermediate output of the map phase.\n\uf0b7 Since the map output is written to disk and transferred across the network to the reducer nodes,\nby using a fast compressor such as LZO, you can get performance gains simply because the\nvolume of data to transfer is reduced.\n\uf0b7 Here are the lines to add to enable gzipmap output compression in your job:\n5.3 Serialization\n\uf0b7 Serialization is the process of turning structured objects into a byte stream for transmission\nover a network or for writing to persistent storage. Deserialization is the process of turning a byte stream back into a\nseries of structured objects.\n\uf0b7 In Hadoop, interprocess communication between nodes in the system is implemented using remote\nprocedure calls(RPCs). The RPC protocol uses serialization to render the message into a binary stream to be\nsent to the remote node, which then deserializes the binary stream into the original message.\nIn general, it is desirable that an RPC serialization format is:\n\uf0d8 Compact: A compact format makes the best use of network bandwidth\n\uf0d8 Fast: Interprocess communication forms the backbone for a distributed system, so it is essential\nthat there is as little performance overhead as possible for the serialization and deserialization process.\n\uf0d8 Extensible: Protocols change over time to meet new requirements, so it should be straightforward to evolve\nthe protocol in a controlled manner for clients and servers.\n\uf0d8 Interoperable : For some systems, it is desirable to be able to support clients that are written in\n\ndifferent languages to the server.\n5.3.1 Writable Interface\n\uf0b7 The Writable interface defines two methods: one for writing its state to a DataOutput binary stream, and one\nfor reading its state from a DataInput binary stream.\n\uf0b7 We will use IntWritable, a wrapper for a Java int. We can create one and set its value using the set() method:\nIntWritable writable = new IntWritable();\nwritable.set(163);\n\uf0b7 To examine the serialized form of the IntWritable, we write a small helper method that wraps a\njava.io.ByteArrayOutputStream in a java.io.DataOutputStream to capture the bytes in the serialized stream\nByteArrayOutputStream out = new ByteArrayOutputStream();\nDataOutputStreamdataOut = new DataOutputStream(out);\nwritable.write(dataOut);\ndataOut.close();\nreturnout.toByteArray();\n5.2.2 Writable Classes\n\uf0b7 Hadoop comes with a large selection of Writable classes in the org.apache.hadoop.io package. They form the\nclass hierarchy shown in Figure 4-1.\nWritable Class\n\uf0b7 Writable wrappers for Java primitives\n\uf0b7 There are Writable wrappers for all the Java primitive types except short and char.\n\uf0b7 All have a get() and a set() method for retrieving and storing the wrapped value.\n\nText\n\uf0b7 Text is a Writable for UTF-8 sequences. It can be thought of as the Writable equivalent of java.lang.String.\n\uf0b7 The Text class uses an int to store the number of bytes in the string encoding, so the maximum value is 2\nGB. Furthermore, Text uses standard UTF -8, which makes it potentially easier to interpoperate with other\ntools that understand UTF-8.\n\uf0b7 The Text class has several features.\n\uf0a7 Indexing\n\uf0a7 Unicode\n\uf0a7 Iteration\n\uf0a7 Mutability\n\uf0a7 Resorting to String\nIndexing\n\uf0b7 Indexing for the Text class is in terms of position in the encoded byte sequence, not the Unicode character in the\nstring, or the Java char code unit. For ASCII String, these three concepts of index position coincide.\n\uf0b7 Notice that charAt() returns an intrepresenting a Unicode code point, unlike the String variant that returns a char.\nText also has a find() method, which is analogous to String\u2019s indexOf()\nUnicode\n\uf0b7 When we start using characters that are encoded with more than a single byte, the differences between Text\nand String become clear. Consider the Unicode characters shown in Table 4-7 All but the last character in the\ntable, U+10400, can be expressed using a single Java\nchar.\nIteration\nIterating over the Unicode characters in Text is complicated by the use of byte offsets for indexing, since\nyou can\u2019t just increment the index.\n\uf0b7 The idiom for iteration is a little obscure: turn the Text object into a java.nio.ByteBuffer. Then repeatedly\ncall the bytesToCodePoint() static method on Text with the buffer. This method extracts the next code point as an\nintand updates the position in the buffer.\nFor Example...\npublic class TextIterator\n{\npublic static void main(String[] args)\n{\nText t = new Text(\"\\u0041\\u00DF\\u6771\\uD801\\uDC00\");\nByteBufferbuf = ByteBuffer.wrap(t.getBytes(), 0, t.getLength());\nIteration. 102 | Chapter 4: Hadoop I/O intcp;\nwhile (buf.hasRemaining() && (cp = Text.bytesToCodePoint(buf)) != -1)\n{\nSystem.out.println(Integer.toHexString(cp));\n}\n}\n}\n\nMutability\nAnother difference with String is that Text is mutable. You can reuse a Text instance by calling on of the set()\nmethods on it.\nFor Example...\nText t = new Text(\"hadoop\");\nt.set(\"pig\");\nassertThat(t.getLength(), is(3));\nassertThat(t.getBytes().length, is(3));\nRestoring to String\n\uf0b7 Text doesn\u2019t have as rich an API for manipulating strings as java.lang.String , so in many cases you need to\nconvert the Text object to a String.\nNull Writable\n\uf0b7 NullWritable is a special type of Writable, as it has a zero -length serialization. No bytes are written to , or\nread from , the stream. It is used as a placeholder.\n\uf0b7 For example, in MapReduce , a key or a value can be declared as a NullWritable when you don\u2019t need to use\nthat position-it effectively stores a constant empty value.\n\uf0b7 NullWritable can also be useful as a key in SequenceFile when you want to store a list of values, as opposed to\nkey-value pairs. It is an immutable singleton: the instance can be retrieved by calling NullWritable.get().\n5.2.4 Serialization Frameworks\no Although most Map Reduce programs use Writable key and value types, this isn\u2019t mandated by the Map\nReduce API. In fact, any types can be used, the only requirement is that there be a mechanism that translates to\nand from a binary representation of each type.\n\uf0b7 To support this, Hadoophas an API for pluggable serialization frameworks. A serialization framework is\nrepresented by an implementation of Serialization. WritableSerialization,for example, is the implementation of\nSerialization for Writable types.\n\uf0b7 Although making it convenient to be able to use standard Java types in Map Reduce programs, like Integer or\nString, Java Object Serialization is not as efficient as Writable, so it\u2019s not worth making this trade-off.\n5.4 File-Based data structure\n\uf0b7 For some applications, you need a specialized data structure to hold your data. For MapReduce-based\nprocessing, putting each blob of binary data into its own file doesn\u2019t scale, so Hadoopdeveloped a number of\nhigher-level containers for these situations.\n\uf0b7 Higher-level containers\no SequenceFile\no MapFile\no\n5.4.1 SequenceFile\n\uf0b7 Imagine a logfile, where each log record is a new line of text. If you want to logbinary types, plain text isn\u2019t a\nsuitable format.\n\uf0b7 Hadoop\u2019sSequenceFileclass fits the bill in this situation, providing a persistent data structure for binary key-\nvalue pairs.To use it as a logfileformat, you would choose a key, such as timestamp represented by a\nLongWritable, and the value is Writable that represents the quantity being logged.\n\uf0b7 SequenceFilealso work well as containers for smaller files. HDFS and Map Reduce are optimized for large\nfiles, so packing files into a SequenceFilemakes storing and processing the smaller files more efficient.\n\nWriting a SequenceFile\n\uf0b7 To create a SequenceFile, use one of its createWriter() static methods, which returns a\nSequenceFile.Writerinstance.\n\uf0b7 The keys and values stored in a SequenceFiledo not necessarily need to be Writable. Any types that can be\nserialized and deserializedby a Serialization may be used.\n\uf0b7 Once you have a SequenceFile.Writer, you then write key-value pairs, using the append() method. Then\nwhen you\u2019ve finished you call the close() method (SequenceFile.Writerimplements java.io.Closeable)\nFor example...\nIntWritable key = new IntWritable();\nText value = new Text();\nSequenceFile.Writer writer = null;\ntry { writer = SequenceFile.createWriter(fs, conf, path, key.getClass(), value.getClass());\nfor (int i = 0; i < 100; i++) { key.set(100 - i); value.set(DATA[i % DATA.length]);\nSystem.out.printf(\"[%s]\\t%s\\t%s\\n\", writer.getLength(), key, value);\nwriter.append(key, value); } } finally { IOUtils.closeStream(writer);\nReading a SequenceFile\nReading sequence files from beginning to end is a matter of creating an instance of SequenceFile.Reader, and\niterating over records by repeatedlyinvoking one of the next() methods.\nIf you are using Writable types, you can use the next() method that takes a key and a value argument, and\nreads the next key and value in the stream into these variables:\nFor example... public static void main(String[] args) throws IOException\n{\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystemfs = FileSystem.get(URI.create(uri), conf);\nPath path = new Path(uri);\nSequenceFile.Reader reader = null;\ntry\n{\nreader = new SequenceFile.Reader(fs, path, conf);\nWritable key = (Writable) ReflectionUtils.newInstance(reader.getKeyClass(), conf);\nWritable value = (Writable) ReflectionUtils.newInstance(reader.getValueClass(), conf);\nlong position = reader.getPosition();\nwhile (reader.next(key, value)) { String syncSeen = reader.syncSeen() ? \"*\" : \"\";\nSystem.out.printf(\"[%s%s]\\t%s\\t%s\\n\", position, syncSeen, key, value);\nposition = reader.getPosition(); // beginning of next record\n}\n}\nFinally\n{\nIOUtils.closeStream(reader);\n}\n\n5.4.2 MapFile\n\uf0b7 A MapFile is a sorted SequenceFile with an index to permit lookups by key. MapFile can be thought of as a\npersistent form of java.util.Map(although it doesn\u2019t implement this interface), which is able to grow beyond\nthe size of a Map that is kept in memory\nWriting a MapFile\n\uf0b7 Writing a MapFile is similar to writing a Sequence File. You create an instance of MapFile. Writer,\nthen call the append () method to add entries in order. Keys must be instances of WritableComparable, and\nvalues must be Writable\nFor example:\nString uri = args[0];\nConfiguration conf = new Configuration();\nFileSystemfs = FileSystem.get(URI.create(uri), conf);\nIntWritable key = new IntWritable();\nText value = new Text();\nMapFile.Writer writer = null;\ntry\n{\nwriter = new MapFile.Writer(conf, fs, uri, key.getClass(), value.getClass());\nfor (int i = 0; i < 1024; i++) { key.set(i + 1);\nvalue.set(DATA[i % DATA.length]);\nwriter.append(key, value);\n}\nFinally\n{\nIOUtils.closeStream(writer);\n}\nReading a MapFile\n\uf0b7 Iterating through the entries in order in a MapFileis similar to the procedure for a SequenceFile. You create a\nMapFile. Reader, then call the next() method until it returns false, signifying that no entry was read because the end\nof the file was reached.\npublicboolean next(WritableComparable key, Writable val) throws IOException\npublic Writable get(WritableComparable key, Writable val) throws IOException\n\n\uf0b7 The return value is used to determine if an entry was found in the MapFile. If it\u2019s null, then no value exist for the\ngiven key. If key was found, then the value for that key is read into val, as well as being returned from the\nmethod call.\n\uf0b7 For this operation, the MapFile. Readerreads the index file into memory. A very large MapFile\u2019s index can\ntake up a lot of memory. Rather than reindex to change the index interval, it is possible to lad only a fraction of the\nindex keys into memory when reading the MapFile by setting the io.amp.index.ksipproperty.\nConverting a SequenceFileto a MapFile\n\uf0b7 One way of looking at a MapFile is as an indexed and sorted SequenceFile. So it\u2019s quite natural to want to be\nable to convert a SequenceFile into a MapFile.\nFor example.\nSequenceFile.Reader reader = new SequenceFile.Reader(fs, mapData, conf);\nClass keyClass = reader.getKeyClass();\nClass valueClass = reader.getValueClass(); reader.close();\n// Create the map file index file\nlong entries = MapFile.fix(fs, map, keyClass, valueClass, false, conf);\nSystem.out.printf(\"Created MapFile %s with %d entries\\n\", map, entries);\nReading Data from a Hadoop URL\nOne of the simplest ways to read a file from a Hadoop filesystem is by using a java.net.URL object to open a\nstream to read the data from. The general idiom is:\nInputStream in = null;\ntry {\nin = new URL(\"hdfs://host/path\").openStream();\n// process in\n} finally {\nIOUtils.closeStream(in);\n}\nThere\u2019s a little bit more work required to make Java recognize Hadoop\u2019s hdfs URL scheme. This is achieved\nby calling the setURLStreamHandlerFactory() method on URL with an instance of FsUrlStreamHandlerFactory.\nThis method can be called only once per JVM, so it is typically executed in a static block. This limitation means that\nif some other part of your program \u2014 perhaps a third-party component outside your control \u2014 sets a\n\nURLStreamHandlerFactory, you won\u2019t be able to use this approach for reading data from Hadoop. The next section\ndiscusses an alternative.\nExample 3-1 shows a program for displaying files from Hadoop filesystems on standard output, like the Unix cat\ncommand.\nExample 3-1. Displaying files from a Hadoop filesystem on standard output using a URLStreamHandler\npublic class URLCat {\nstatic {\nURL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\n}\npublic static void main(String[] args) throws Exception {\nInputStream in = null;\ntry {\nin = new URL(args[0]).openStream();\nIOUtils.copyBytes(in, System.out, 4096, false);\n}\nfinally {\nIOUtils.closeStream(in); } } }\nWe make use of the handy IOUtils class that comes with Hadoop for closing the stream in the finally clause,\nand also for copying bytes between the input stream and the output stream (System.out, in this case). The last two\narguments to the copyBytes() method are the buffer size used for copying and whether to close the streams when the\ncopy is complete. We close the input stream ourselves, and System.out doesn\u2019t need to be closed.\nHere\u2019s a sample run:[31]\n% export HADOOP_CLASSPATH=hadoop-examples.jar\n% hadoop URLCat hdfs://localhost/user/tom/quangle.txt\nOn the top of the Crumpetty Tree\nThe Quangle Wangle sat,\nBut his face you could not see,\nOn account of his Beaver Hat.\n\nRead Operation In HDFS\nData read request is served by HDFS, NameNode, and DataNode. Let's call the reader as a 'client'. Below diagram\ndepicts file read operation in Hadoop.\n1. A client initiates read request by calling 'open()' method of FileSystem object; it is an object of\ntype DistributedFileSystem.\n2. This object connects to namenode using RPC and gets metadata information such as the locations of the\nblocks of the file. Please note that these addresses are of first few blocks of a file.\n3. In response to this metadata request, addresses of the DataNodes having a copy of that block is returned back.\n4. Once addresses of DataNodes are received, an object of type FSDataInputStream is returned to the\nclient. FSDataInputStream contains DFSInputStream which takes care of interactions with DataNode and\nNameNode. In step 4 shown in the above diagram, a client invokes 'read()' method which\ncauses DFSInputStream to establish a connection with the first DataNode with the first block of a file.\n5. Data is read in the form of streams wherein client invokes 'read()' method repeatedly. This process\nof read() operation continues till it reaches the end of block.\n6. Once the end of a block is reached, DFSInputStream closes the connection and moves on to locate the next\nDataNode for the next block\n7. Once a client has done with the reading, it calls a close() method.\n\nWrite Operation In HDFS\nIn this,we know how data is written into HDFS through files.\n1. A client initiates write operation by calling 'create()' method of DistributedFileSystem object which creates a\nnew file - Step no. 1 in the above diagram.\n2. DistributedFileSystem object connects to the NameNode using RPC call and initiates new file creation.\nHowever, this file creates operation does not associate any blocks with the file. It is the responsibility of\nNameNode to verify that the file (which is being created) does not exist already and a client has correct\npermissions to create a new file. If a file already exists or client does not have sufficient permission to create a\nnew file, then IOException is thrown to the client. Otherwise, the operation succeeds and a new record for\nthe file is created by the NameNode.\n3. Once a new record in NameNode is created, an object of type FSDataOutputStream is returned to the client.\nA client uses it to write data into the HDFS. Data write method is invoked (step 3 in the diagram).\n4. FSDataOutputStream contains DFSOutputStream object which looks after communication with DataNodes\nand NameNode. While the client continues writing data, DFSOutputStream continues creating packets with\nthis data. These packets are enqueued into a queue which is called as DataQueue.\n5. There is one more component called DataStreamer which consumes this DataQueue. DataStreamer also asks\nNameNode for allocation of new blocks thereby picking desirable DataNodes to be used for replication.\n6. Now, the process of replication starts by creating a pipeline using DataNodes. In our case, we have chosen a\nreplication level of 3 and hence there are 3 DataNodes in the pipeline.\n7. The DataStreamer pours packets into the first DataNode in the pipeline.\n8. Every DataNode in a pipeline stores packet received by it and forwards the same to the second DataNode in a\npipeline.\n9. Another queue, 'Ack Queue' is maintained by DFSOutputStream to store packets which are waiting for\nacknowledgment from DataNodes.\n10. Once acknowledgment for a packet in the queue is received from all DataNodes in the pipeline, it is removed\nfrom the 'Ack Queue'. In the event of any DataNode failure, packets from this queue are used to reinitiate the\noperation.\n11. After a client is done with the writing data, it calls a close() method (Step 9 in the diagram) Call to close(),\nresults into flushing remaining data packets to the pipeline followed by waiting for acknowledgment.\n12. Once a final acknowledgment is received, NameNode is contacted to tell it that the file write operation is\ncomplete.\n\nAccess HDFS using JAVA API\nIn order to interact with Hadoop's filesystem programmatically, Hadoop provides multiple JAVA classes.\nPackage named org.apache.hadoop.fs contains classes useful in manipulation of a file in Hadoop's filesystem. These\noperations include, open, read, write, and close. Actually, file API for Hadoop is generic and can be extended to\ninteract with other filesystems other than HDFS.\nReading a file from HDFS, programmatically\nObject java.net.URL is used for reading contents of a file. To begin with, we need to make Java recognize Hadoop's\nhdfs URL scheme. This is done by calling setURLStreamHandlerFactory method on URL object and an instance of\nFsUrlStreamHandlerFactory is passed to it. This method needs to be executed only once per JVM, hence it is\nenclosed in a static block.\nAn example code is-\npublic class URLCat {\nstatic {\nURL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());\n}\npublic static void main(String[] args) throws Exception {\nInputStream in = null;\ntry {\nin = new URL(args[0]).openStream();\nIOUtils.copyBytes(in, System.out, 4096, false);\n} finally {\nIOUtils.closeStream(in);\n}\n}\n}\nThis code opens and reads contents of a file. Path of this file on HDFS is passed to the program as a command line\nargument.\nAccess HDFS Using COMMAND-LINE INTERFACE\nThis is one of the simplest ways to interact with HDFS. Command-line interface has support for filesystem\noperations like read the file, create directories, moving files, deleting data, and listing directories.\nWe can run '$HADOOP_HOME/bin/hdfs dfs -help' to get detailed help on every command. Here, 'dfs' is a shell\ncommand of HDFS which supports multiple subcommands.\nSome of the widely used commands are listed below along with some details of each one.\n1. Copy a file from the local filesystem to HDFS\n$HADOOP_HOME/bin/hdfs dfs -copyFromLocal temp.txt /\nThis command copies file temp.txt from the local filesystem to HDFS.\n\n2. We can list files present in a directory using -ls\n$HADOOP_HOME/bin/hdfs dfs -ls /\nWe can see a file 'temp.txt' (copied earlier) being listed under ' / ' directory.\n3. Command to copy a file to the local filesystem from HDFS\n$HADOOP_HOME/bin/hdfs dfs -copyToLocal /temp.txt\nWe can see temp.txt copied to a local filesystem.\n4. Command to create a new directory\n$HADOOP_HOME/bin/hdfs dfs -mkdir /mydirectory\nStarting HDFS\nInitially you have to format the configured HDFS file system, open namenode (HDFS server), and execute the\nfollowing command.\n$ hadoop namenode -format\nAfter formatting the HDFS, start the distributed file system. The following command will start the namenode as\nwell as the data nodes as cluster.\n$ start-dfs.sh\nListing Files in HDFS\nAfter loading the information in the server, we can find the list of files in a directory, status of a file, using \u2018ls\u2019. Given\nbelow is the syntax of ls that you can pass to a directory or a filename as an argument.\n$ $HADOOP_HOME/bin/hadoop fs -ls <args>\n\nInserting Data into HDFS\nAssume we have data in the file called file.txt in the local system which is ought to be saved in the hdfs file system.\nFollow the steps given below to insert the required file in the Hadoop file system.\nStep 1\nYou have to create an input directory.\n$ $HADOOP_HOME/bin/hadoop fs -mkdir /user/input\nStep 2\nTransfer and store a data file from local systems to the Hadoop file system using the put command.\n$ $HADOOP_HOME/bin/hadoop fs -put /home/file.txt /user/input\nStep 3\nYou can verify the file using ls command.\n$ $HADOOP_HOME/bin/hadoop fs -ls /user/input\nRetrieving Data from HDFS\nAssume we have a file in HDFS called outfile. Given below is a simple demonstration for retrieving the required file\nfrom the Hadoop file system.\nStep 1\nInitially, view the data from HDFS using cat command.\n$ $HADOOP_HOME/bin/hadoop fs -cat /user/output/outfile\nStep 2\nGet the file from HDFS to the local file system using get command.\n$ $HADOOP_HOME/bin/hadoop fs -get /user/output/ /home/hadoop_tp/\nShutting Down the HDFS\nYou can shut down the HDFS by using the following command.\n$ stop-dfs.sh\n1. Create a directory in HDFS at given path(s).\nUsage:\nhadoop fs -mkdir <paths>\nExample:\nhadoop fs -mkdir /user/saurzcode/dir1 /user/saurzcode/dir2\n\n2. List the contents of a directory.\nUsage :\nhadoop fs -ls <args>\nExample:\nhadoop fs -ls /user/saurzcode\n3. Upload and download a file in HDFS.\nUpload:\nhadoop fs -put:\nCopy single src file, or multiple src files from local file system to the Hadoop data file system\nUsage:\nhadoop fs -put <localsrc> ... <HDFS_dest_Path>\nExample:\nhadoop fs -put /home/saurzcode/Samplefile.txt /user/saurzcode/dir3/\nDownload:\nhadoop fs -get:\nCopies/Downloads files to the local file system\nUsage:\nhadoop fs -get <hdfs_src> <localdst>\nExample:\nhadoop fs -get /user/saurzcode/dir3/Samplefile.txt /home/\n4. See contents of a file\nSame as unix cat command:\nUsage:\nhadoop fs -cat <path[filename]>\nExample:\nhadoop fs -cat /user/saurzcode/dir1/abc.txt\n5. Copy a file from source to destination\nThis command allows multiple sources as well in which case the destination must be a directory.\nUsage:\nhadoop fs -cp <source> <dest>\nExample:\nhadoop fs -cp /user/saurzcode/dir1/abc.txt /user/saurzcode/dir2\n6. Copy a file from/To Local file system to HDFS\ncopyFromLocal\nUsage:\nhadoop fs -copyFromLocal <localsrc> URI\n\nExample:\nhadoop fs -copyFromLocal /home/saurzcode/abc.txt /user/saurzcode/abc.txt\nSimilar to put command, except that the source is restricted to a local file reference.\ncopyToLocal\nUsage:\nhadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst>\nSimilar to get command, except that the destination is restricted to a local file reference.\n7. Move file from source to destination.\nNote:- Moving files across filesystem is not permitted.\nUsage :\nhadoop fs -mv <src> <dest>\nExample:\nhadoop fs -mv /user/saurzcode/dir1/abc.txt /user/saurzcode/dir2\n8. Remove a file or directory in HDFS.\nRemove files specified as argument. Deletes directory only when it is empty\nUsage :\nhadoop fs -rm <arg>\nExample:\nhadoop fs -rm /user/saurzcode/dir1/abc.txt\nRecursive version of delete.\nUsage :\nhadoop fs -rmr <arg>\nExample:\nhadoop fs -rmr /user/saurzcode/\n9. Display last few lines of a file.\nSimilar to tail command in Unix.\nUsage :\nhadoop fs -tail <path[filename]>\nExample:\nhadoop fs -tail /user/saurzcode/dir1/abc.txt\n10. Display the aggregate length of a file.\nUsage :\nhadoop fs -du <path>\nExample:\nhadoop fs -du /user/saurzcode/dir1/abc.txt\n\n",
      "sections": [
        "Characteristics (OR) Features of Cloud Environments:",
        "On-demand self-service: This means that cloud customers can sign up for, pay for and",
        "Broad network access: Customers access cloud services via the Internet.",
        "Resource pooling: Many different customers (individuals, organizations or different",
        "Rapid elasticity or expansion: Cloud customers can easily scale their use of resources up",
        "Measured service: Customers pay for the amount of resources they use in a given period",
        "Applications:",
        "Cloud Components:",
        "Benefits and Limitations of Cloud Computing",
        "Architecture"
      ],
      "page_start": null,
      "page_end": null
    }
  ],
  "uploaded_at": "2026-01-02T08:48:41.579257"
}